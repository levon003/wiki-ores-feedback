{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Online Vectorization\n",
    "===\n",
    "\n",
    " - Identify vocab from word and document count files\n",
    " - Construct appropriate text vectorizers (and TF-IDF transformers if desired)\n",
    " - Stream ldjson file with tokenized texts in batches, appending each batch as a sparse matrix to the result\n",
    " - Save the result periodically, ultimately resulting in a large sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import bz2\n",
    "import sqlite3\n",
    "import difflib\n",
    "import gzip\n",
    "import json\n",
    "import base64\n",
    "import pickle\n",
    "import re\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "import nltk\n",
    "import scipy.stats\n",
    "import para\n",
    "from itertools import groupby\n",
    "from collections import Counter, defaultdict\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deltas\n",
    "from deltas.tokenizers import wikitext_split\n",
    "from deltas import segment_matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics\n",
    "import sklearn.calibration\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/export/scratch2/levon003/repos/wiki-ores-feedback'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "git_root_dir = !git rev-parse --show-toplevel\n",
    "git_root_dir = git_root_dir[0]\n",
    "git_root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/export/scratch2/wiki_data',\n",
       " '/export/scratch2/levon003/repos/wiki-ores-feedback/data/derived')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_dir = \"/export/scratch2/wiki_data\"\n",
    "derived_data_dir = os.path.join(git_root_dir, \"data\", \"derived\")\n",
    "raw_data_dir, derived_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/export/scratch2/levon003/repos/wiki-ores-feedback/data/derived/stub-history-all-revisions'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stub_history_dir = os.path.join(derived_data_dir, 'stub-history-all-revisions')\n",
    "stub_history_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/export/scratch2/levon003/repos/wiki-ores-feedback/data/derived/audit'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revision_sample_dir = os.path.join(derived_data_dir, 'revision_sample')\n",
    "working_dir = os.path.join(derived_data_dir, 'audit')\n",
    "working_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 3 data loaded in 0:00:35.499775.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33964442"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the sample dataframe\n",
    "s = datetime.now()\n",
    "revision_sample_dir = os.path.join(derived_data_dir, 'revision_sample')\n",
    "sample3_filepath = os.path.join(revision_sample_dir, 'sample3_all.pkl')\n",
    "rev_df = pd.read_pickle(sample3_filepath)\n",
    "print(f\"Sample 3 data loaded in {datetime.now() - s}.\")\n",
    "len(rev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_id</th>\n",
       "      <th>rev_id</th>\n",
       "      <th>rev_timestamp</th>\n",
       "      <th>is_revert_target</th>\n",
       "      <th>is_reverted</th>\n",
       "      <th>is_reverting</th>\n",
       "      <th>is_sample_eligible</th>\n",
       "      <th>prev_rev_id</th>\n",
       "      <th>next_rev_id</th>\n",
       "      <th>prev_rev_timestamp</th>\n",
       "      <th>next_rev_timestamp</th>\n",
       "      <th>reverted_rev_ids</th>\n",
       "      <th>reverting_rev_id</th>\n",
       "      <th>reverting_rev_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>818613649</td>\n",
       "      <td>1515102279</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>818611292</td>\n",
       "      <td>818624114</td>\n",
       "      <td>1515101356</td>\n",
       "      <td>1515106953</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>818624114</td>\n",
       "      <td>1515106953</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>818613649</td>\n",
       "      <td>820024812</td>\n",
       "      <td>1515102279</td>\n",
       "      <td>1515798752</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>820024812</td>\n",
       "      <td>1515798752</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>818624114</td>\n",
       "      <td>820025687</td>\n",
       "      <td>1515106953</td>\n",
       "      <td>1515799060</td>\n",
       "      <td>[]</td>\n",
       "      <td>820025687</td>\n",
       "      <td>1515799060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>820025687</td>\n",
       "      <td>1515799060</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>820024812</td>\n",
       "      <td>820703495</td>\n",
       "      <td>1515798752</td>\n",
       "      <td>1516095884</td>\n",
       "      <td>[820024812]</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>820703495</td>\n",
       "      <td>1516095884</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>820025687</td>\n",
       "      <td>821673418</td>\n",
       "      <td>1515799060</td>\n",
       "      <td>1516597634</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_id     rev_id  rev_timestamp  is_revert_target  is_reverted  \\\n",
       "1       12  818613649     1515102279                 0            0   \n",
       "2       12  818624114     1515106953                 1            0   \n",
       "3       12  820024812     1515798752                 0            1   \n",
       "4       12  820025687     1515799060                 0            0   \n",
       "5       12  820703495     1516095884                 0            0   \n",
       "\n",
       "   is_reverting  is_sample_eligible  prev_rev_id  next_rev_id  \\\n",
       "1             0                True    818611292    818624114   \n",
       "2             0                True    818613649    820024812   \n",
       "3             0                True    818624114    820025687   \n",
       "4             1                True    820024812    820703495   \n",
       "5             0                True    820025687    821673418   \n",
       "\n",
       "   prev_rev_timestamp  next_rev_timestamp reverted_rev_ids  reverting_rev_id  \\\n",
       "1          1515101356          1515106953               []                -1   \n",
       "2          1515102279          1515798752               []                -1   \n",
       "3          1515106953          1515799060               []         820025687   \n",
       "4          1515798752          1516095884      [820024812]                -1   \n",
       "5          1515799060          1516597634               []                -1   \n",
       "\n",
       "   reverting_rev_timestamp  \n",
       "1                       -1  \n",
       "2                       -1  \n",
       "3               1515799060  \n",
       "4                       -1  \n",
       "5                       -1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load texts into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_dir = os.path.join(derived_data_dir, 'audit')\n",
    "text_db_filepath = os.path.join(audit_dir, 'text_2020-07-23T13:08:38Z.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db(db_filename):\n",
    "    db = sqlite3.connect(\n",
    "            db_filename,\n",
    "            detect_types=sqlite3.PARSE_DECLTYPES\n",
    "        )\n",
    "    db.row_factory = sqlite3.Row\n",
    "    return db\n",
    "\n",
    "def get_existing_rev_ids(db_filepath):\n",
    "    rev_ids = set()\n",
    "    try:\n",
    "        db = get_db(db_filepath)\n",
    "        cursor = db.execute(\"SELECT rev_id FROM revisionText\")\n",
    "        for result in cursor:\n",
    "            rev_id = result['rev_id']\n",
    "            rev_ids.add(rev_id)\n",
    "    finally:\n",
    "        db.close()\n",
    "    return rev_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1106018/1106018 [02:25<00:00, 7577.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1106018"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text_dict_list = []\n",
    "rev_id_content_dict = {}\n",
    "rev_id_comment_dict = {}\n",
    "try:\n",
    "    db = get_db(text_db_filepath)\n",
    "    cursor = db.execute(\"SELECT rev_id, content, comment FROM revisionText\")\n",
    "    for result in tqdm(cursor, total=1106018):\n",
    "        rev_id = result['rev_id']\n",
    "        rev_id_content_dict[rev_id] = result['content']\n",
    "        rev_id_comment_dict[rev_id] = result['comment']\n",
    "        #comment = result['comment']\n",
    "        #content = result['content']\n",
    "        #text_dict_list.append({\n",
    "        #    'rev_id': rev_id,\n",
    "        #    'content': content,\n",
    "        #    'comment': comment\n",
    "        #})\n",
    "finally:\n",
    "    db.close()\n",
    "len(rev_id_content_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1106018"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_ids_with_text = get_existing_rev_ids(text_db_filepath)\n",
    "len(rev_ids_with_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_df = pd.DataFrame(text_dict_list)\n",
    "#print(len(text_df))\n",
    "#text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add text availability to sample3 revision data\n",
    "\n",
    "Either join in a dataframe with the text data or just record which entries have text available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.merge(rev_df, text_df, how='left', on='rev_id')\n",
    "df = rev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1106018, 0.032563997371133024)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['has_text'] = ~df.content.isna()\n",
    "df['has_text'] = df.rev_id.map(lambda rev_id: rev_id in rev_id_content_dict)\n",
    "np.sum(df.has_text), np.sum(df.has_text) / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_ids_with_text = set(df[df.has_text].rev_id)\n",
    "df['prev_rev_has_text'] = df.prev_rev_id.map(lambda rev_id: rev_id in rev_ids_with_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(996951, 0.029352786069619517)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(df.prev_rev_has_text), np.sum(df.prev_rev_has_text) / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "689050"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((df.prev_rev_has_text)&(df.has_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mess around with creating some features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "689050"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf = df[(df.prev_rev_has_text)&(df.has_text)]\n",
    "len(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(154493, 154373)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_rev_id = sdf.iloc[0].prev_rev_id\n",
    "curr_rev_id = sdf.iloc[0].rev_id\n",
    "prev_content = rev_id_content_dict[prev_rev_id]\n",
    "curr_content = rev_id_content_dict[curr_rev_id]\n",
    "len(prev_content), len(curr_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_tokens = wikitext_split.tokenize(prev_content)\n",
    "curr_tokens = wikitext_split.tokenize(curr_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Equal(name='equal', a1=0, a2=34701, b1=0, b2=34701), Delete(name='delete', a1=34701, a2=34712, b1=34701, b2=34701), Equal(name='equal', a1=34712, a2=34716, b1=34701, b2=34705), Delete(name='delete', a1=34716, a2=34717, b1=34705, b2=34705), Insert(name='insert', a1=34717, a2=34717, b1=34705, b2=34706), Equal(name='equal', a1=34717, a2=34718, b1=34706, b2=34707), Delete(name='delete', a1=34718, a2=34719, b1=34707, b2=34707), Equal(name='equal', a1=34719, a2=34727, b1=34707, b2=34715), Delete(name='delete', a1=34727, a2=34730, b1=34715, b2=34715), Equal(name='equal', a1=34730, a2=34731, b1=34715, b2=34716), Delete(name='delete', a1=34731, a2=34758, b1=34716, b2=34716), Equal(name='equal', a1=34758, a2=51796, b1=34716, b2=51754)]\n"
     ]
    }
   ],
   "source": [
    "print(list(segment_matcher.diff(prev_tokens, curr_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Token('According', type='word'),\n",
       " Token(' ', type='whitespace'),\n",
       " Token('to', type='word'),\n",
       " Token(' ', type='whitespace'),\n",
       " Token('Cochrane', type='word'),\n",
       " Token(' ', type='whitespace'),\n",
       " Token('review', type='word'),\n",
       " Token(' ', type='whitespace'),\n",
       " Token('2018', type='number'),\n",
       " Token(',', type='comma'),\n",
       " Token(' ', type='whitespace'),\n",
       " Token('that', type='word'),\n",
       " Token('\"', type='etc'),\n",
       " Token('(', type='paren_open'),\n",
       " Token('EIBI', type='word'),\n",
       " Token(')', type='paren_close'),\n",
       " Token(' ', type='whitespace'),\n",
       " Token('an', type='word'),\n",
       " Token(' ', type='whitespace'),\n",
       " Token('effective', type='word'),\n",
       " Token(' ', type='whitespace'),\n",
       " Token('treatment', type='word'),\n",
       " Token(' ', type='whitespace'),\n",
       " Token('for', type='word'),\n",
       " Token(' ', type='whitespace'),\n",
       " Token('some', type='word'),\n",
       " Token(' ', type='whitespace'),\n",
       " Token('children', type='word'),\n",
       " Token(' ', type='whitespace'),\n",
       " Token('with', type='word'),\n",
       " Token(' ', type='whitespace'),\n",
       " Token('ASD', type='word'),\n",
       " Token('\"', type='etc'),\n",
       " Token(' ', type='whitespace'),\n",
       " Token('is', type='word'),\n",
       " Token(' ', type='whitespace'),\n",
       " Token('considered', type='word'),\n",
       " Token(' ', type='whitespace'),\n",
       " Token('low', type='word'),\n",
       " Token(' ', type='whitespace'),\n",
       " Token('or', type='word'),\n",
       " Token(' ', type='whitespace'),\n",
       " Token('very', type='word')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_removed_tokens = []\n",
    "all_inserted_tokens = []\n",
    "for segment in segment_matcher.diff(prev_tokens, curr_tokens):\n",
    "    if segment.name == 'equal':\n",
    "        continue\n",
    "    elif segment.name == 'delete':\n",
    "        removed_tokens = prev_tokens[segment.a1:segment.a2]\n",
    "        #print(' '.join(removed_tokens))\n",
    "        all_removed_tokens.extend(removed_tokens)\n",
    "    elif segment.name == 'insert':\n",
    "        inserted_tokens = curr_tokens[segment.b1:segment.b2]\n",
    "        #print(' '.join(inserted_tokens))\n",
    "        all_inserted_tokens.extend(inserted_tokens)\n",
    "    else:\n",
    "        raise ValueError('I do not think substitutitions are implemented...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5439, 5439, 5439)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = Counter(curr_tokens)\n",
    "curr_counter = Counter(curr_tokens)\n",
    "prev_counter = Counter(prev_tokens)\n",
    "diff.subtract(prev_counter)\n",
    "len(diff), len(curr_counter), len(prev_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Token(' ', type='whitespace')\t-18\n",
      "                  Token('\"', type='etc')\t-2\n",
      "           Token('(', type='paren_open')\t-1\n",
      "          Token(')', type='paren_close')\t-1\n",
      "            Token('2018', type='number')\t-1\n",
      "                Token('or', type='word')\t-1\n",
      "                Token('is', type='word')\t-1\n",
      "              Token('with', type='word')\t-1\n",
      "                Token(',', type='comma')\t-1\n",
      "         Token('treatment', type='word')\t-1\n",
      "              Token('some', type='word')\t-1\n",
      "          Token('children', type='word')\t-1\n",
      "               Token('ASD', type='word')\t-1\n",
      "            Token('review', type='word')\t-1\n",
      "                Token('to', type='word')\t-1\n",
      "              Token('that', type='word')\t-1\n",
      "                Token('an', type='word')\t-1\n",
      "        Token('considered', type='word')\t-1\n",
      "              Token('very', type='word')\t-1\n",
      "               Token('low', type='word')\t-1\n",
      "         Token('effective', type='word')\t-1\n",
      "         Token('According', type='word')\t-1\n",
      "              Token('EIBI', type='word')\t-1\n",
      "          Token('Cochrane', type='word')\t-1\n"
     ]
    }
   ],
   "source": [
    "for token, count in diff.items():\n",
    "    if count != 0:\n",
    "        print(f\"{repr(token):>40}\\t{count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 8262/689050 [08:29<11:39:33, 16.22it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10001"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_id_tokens_dict = {}\n",
    "c = 0\n",
    "MAX_TEXTS = 10000\n",
    "for row in tqdm(sdf.itertuples(), total=len(sdf)):\n",
    "    prev_rev_id = row.prev_rev_id\n",
    "    curr_rev_id = row.rev_id\n",
    "    if prev_rev_id not in rev_id_tokens_dict:\n",
    "        prev_content = rev_id_content_dict[prev_rev_id]\n",
    "        rev_id_tokens_dict[prev_rev_id] = wikitext_split.tokenize(prev_content)\n",
    "        c += 1\n",
    "    if curr_rev_id not in rev_id_tokens_dict:\n",
    "        curr_content = rev_id_content_dict[curr_rev_id]\n",
    "        rev_id_tokens_dict[curr_rev_id] = wikitext_split.tokenize(curr_content)\n",
    "        c += 1\n",
    "    if c >= MAX_TEXTS:\n",
    "        break\n",
    "len(rev_id_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10001/10001 [00:45<00:00, 219.15it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "608805"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = Counter()\n",
    "for rev_id, tokens in tqdm(rev_id_tokens_dict.items(), total=len(rev_id_tokens_dict)):\n",
    "    word_counts.update(tokens)\n",
    "len(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Token(' ', type='whitespace'), 96745876),\n",
       " (Token('|', type='bar'), 12301610),\n",
       " (Token('=', type='equals'), 7189727),\n",
       " (Token(',', type='comma'), 6356344),\n",
       " (Token('.', type='period'), 5404204),\n",
       " (Token('the', type='word'), 4853522),\n",
       " (Token(']]', type='dbrack_close'), 4727430),\n",
       " (Token('[[', type='dbrack_open'), 4727351),\n",
       " (Token('of', type='word'), 3405190),\n",
       " (Token('\\n', type='whitespace'), 3273284),\n",
       " (Token('-', type='etc'), 2802844),\n",
       " (Token('and', type='word'), 2526688),\n",
       " (Token('}}', type='dcurly_close'), 1947002),\n",
       " (Token('{{', type='dcurly_open'), 1945417),\n",
       " (Token('in', type='word'), 1931983),\n",
       " (Token(\"''\", type='italic'), 1592096),\n",
       " (Token('to', type='word'), 1584792),\n",
       " (Token('\"', type='etc'), 1481191),\n",
       " (Token(')', type='paren_close'), 1379058),\n",
       " (Token('(', type='paren_open'), 1377848)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50895"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([1 for v in word_counts.values() if v >= 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 689050/689050 [00:02<00:00, 263890.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8263"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_rev_ids = set()\n",
    "for row in tqdm(sdf.itertuples(), total=len(sdf)):\n",
    "    prev_rev_id = row.prev_rev_id\n",
    "    curr_rev_id = row.rev_id\n",
    "    if prev_rev_id in rev_id_tokens_dict and curr_rev_id in rev_id_tokens_dict:\n",
    "        labeled_rev_ids.add(curr_rev_id)\n",
    "len(labeled_rev_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_rev_id_dict = {row.rev_id: row.prev_rev_id for row in sdf.itertuples()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50895"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = len([1 for v in word_counts.values() if v >= 100])\n",
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50895"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_index_dict = {tup[0]: i for i, tup in enumerate(word_counts.most_common(n_features))}\n",
    "len(token_index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8263/8263 [01:28<00:00, 93.58it/s] \n"
     ]
    }
   ],
   "source": [
    "X = np.zeros((len(labeled_rev_ids),n_features))\n",
    "for row, curr_rev_id in tqdm(enumerate(labeled_rev_ids), total=len(labeled_rev_ids)):\n",
    "    prev_rev_id = prev_rev_id_dict[rev_id]\n",
    "    prev_tokens = rev_id_tokens_dict[prev_rev_id]\n",
    "    curr_tokens = rev_id_tokens_dict[curr_rev_id]\n",
    "    diff = Counter(curr_tokens)\n",
    "    prev_counter = Counter(prev_tokens)\n",
    "    diff.subtract(prev_counter)\n",
    "    \n",
    "    for token, count in diff.items():\n",
    "        if count != 0 and word_counts[token] >= 100:\n",
    "            X[row,token_index_dict[token]] = count\n",
    "    X[row,:] /= max(len(curr_tokens), len(prev_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8263, 50895)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8263,)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_reverted_dict = {row.rev_id: row.is_reverted == 1 for row in sdf.itertuples()}\n",
    "y = np.array([is_reverted_dict[rev_id] for rev_id in labeled_rev_ids])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599, 0.19351325184557666)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y), np.sum(y) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "382788226"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(X == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9102185867525333"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 91% of entries are 0\n",
    "382788226 / (8263 * 50895)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = sklearn.linear_model.LogisticRegression(\n",
    "    penalty='l2',\n",
    "    C=1.0,\n",
    "    solver='lbfgs'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.20, random_state=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()\n",
      "0:00:05.927926\n"
     ]
    }
   ],
   "source": [
    "s = datetime.now()\n",
    "print(clf)\n",
    "\n",
    "    \n",
    "# train the model\n",
    "md = clf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"{datetime.now() - s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict with the model\n",
    "y_pred_test = md.predict(X_test)\n",
    "y_pred_test_proba = md.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8039927404718693"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_test == y_pred_test) / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5690635769956061"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc = sklearn.metrics.roc_auc_score(y_test, y_pred_test_proba)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/scratch2/levon003/bin/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct the vocabulary on all of the text documents\n",
    "# this should only include TRAINING documents, not TESTING documents\n",
    "s = datetime.now()\n",
    "\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "        tokenizer=dummy,\n",
    "        preprocessor=dummy,\n",
    "        max_features=40000\n",
    "    )\n",
    "\n",
    "count_vectorizer.fit(rev_id_tokens_dict.values())\n",
    "print(f\"{datetime.now() - s}\")\n",
    "\n",
    "# this is the size of the vocabulary\n",
    "len(count_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8263/8263 [00:00<00:00, 474916.88it/s]\n"
     ]
    }
   ],
   "source": [
    "X_docs = []\n",
    "for curr_rev_id in tqdm(labeled_rev_ids):\n",
    "    X_docs.append(rev_id_tokens_dict[curr_rev_id])\n",
    "X = count_vectorizer.transform(X_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.231109\n"
     ]
    }
   ],
   "source": [
    "s = datetime.now()\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf.fit(X)\n",
    "print(f\"{datetime.now() - s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:02:03.295813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/scratch2/levon003/bin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.20, random_state=500)\n",
    "s = datetime.now()\n",
    "\n",
    "clf = sklearn.linear_model.LogisticRegression(\n",
    "    penalty='l2',\n",
    "    C=1.0,\n",
    "    solver='lbfgs',\n",
    "    max_iter=1000\n",
    ")\n",
    "    \n",
    "# train the model\n",
    "md = clf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"{datetime.now() - s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.09618874773139746, 0.7900786448880823, 0.7026598017631376)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test = md.predict(X_test)\n",
    "y_pred_test_proba = md.predict_proba(X_test)[:,1]\n",
    "\n",
    "pct_predicted_reverted = np.sum(y_pred_test) / len(y_pred_test)\n",
    "test_acc = np.sum(y_test == y_pred_test) / len(y_test)\n",
    "roc_auc = sklearn.metrics.roc_auc_score(y_test, y_pred_test_proba)\n",
    "pct_predicted_reverted, test_acc, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x40000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2918 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with diff features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 90499/1106018 [03:28<39:00, 433.94it/s]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50000, 40500)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_list = []\n",
    "diff_json_filepath = os.path.join(audit_dir, 'diff_2020-07-23T13:08:38Z.ldjson')\n",
    "skip_count = 0\n",
    "with open(diff_json_filepath, 'r') as infile:\n",
    "    for line in tqdm(infile, total=len(rev_ids_with_text)):\n",
    "        if np.random.random() >= 0.55:\n",
    "            skip_count += 1\n",
    "            continue\n",
    "        diff = json.loads(line)\n",
    "        diff_list.append(diff)\n",
    "        if len(diff_list) >= 50000:  # optional early-stopping condition to reduce concurrently loaded data size\n",
    "            break\n",
    "len(diff_list), skip_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1106018it [00:03, 290860.39it/s]\n"
     ]
    }
   ],
   "source": [
    "rev_id_is_reverted_dict = {row.rev_id: row.is_reverted for row in tqdm(rev_df[rev_df.rev_id.isin(rev_ids_with_text)].itertuples())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 154547.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# add reverting information to the diff list\n",
    "# optionally, can also bring in the content text from the text database\n",
    "should_add_content_text = False\n",
    "for diff in tqdm(diff_list):\n",
    "    rev_id = diff['rev_id']\n",
    "    diff['is_reverted'] = rev_id_is_reverted_dict[rev_id]\n",
    "    if should_add_content_text:\n",
    "        try:\n",
    "            db = get_db(text_db_filepath)\n",
    "\n",
    "            cursor = db.execute(\"SELECT rev_id, content FROM revisionText WHERE rev_id = ?\", (rev_id,))\n",
    "            result = cursor.fetchall()\n",
    "            if len(result) > 1:\n",
    "                raise ValueError(\"WARNING: Duplicated rev_id in database, check integrity.\")\n",
    "            if len(result) == 0:\n",
    "                raise ValueError(f\"Failed to find rev_id {rev_id} in database.\")\n",
    "            result = result[0]\n",
    "            curr_content = result['content']\n",
    "        finally:\n",
    "            db.close()\n",
    "        diff['content'] = curr_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute odds ratios to identify representative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1106018it [00:05, 207166.81it/s]\n"
     ]
    }
   ],
   "source": [
    "rev_id_is_reverted_dict = {row.rev_id: row.is_reverted for row in tqdm(rev_df[rev_df.rev_id.isin(rev_ids_with_text)].itertuples())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 385982/1106018 [37:39<1:10:14, 170.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content tokens: 8143191 (reverted 2910542)\n",
      "Removed tokens: 499315 (reverted 227893)\n",
      "Inserted tokens: 592580 (reverted 143858)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# compute counts for the reverted reverts only, in order to compute Odds Ratios\n",
    "# oc = occurrence count (document frequency)\n",
    "content_oc = Counter()\n",
    "removed_oc = Counter()\n",
    "inserted_oc = Counter()\n",
    "reverted_content_oc = Counter()\n",
    "reverted_removed_oc = Counter()\n",
    "reverted_inserted_oc = Counter()\n",
    "\n",
    "diff_json_filepath = os.path.join(audit_dir, 'diff_2020-07-23T13:08:38Z.ldjson')\n",
    "with open(diff_json_filepath, 'r') as infile:\n",
    "    for line in tqdm(infile, total=len(rev_ids_with_text)):\n",
    "        diff = json.loads(line)\n",
    "        content_set = set(diff['content_tokens'])\n",
    "        removed_set = set(diff['removed_tokens'])\n",
    "        inserted_set = set(diff['inserted_tokens'])\n",
    "        content_oc.update(content_set)\n",
    "        removed_oc.update(removed_set)\n",
    "        inserted_oc.update(inserted_set)\n",
    "        if rev_id_is_reverted_dict[diff['rev_id']] == 1:\n",
    "            reverted_content_oc.update(content_set)\n",
    "            reverted_removed_oc.update(removed_set)\n",
    "            reverted_inserted_oc.update(inserted_set)\n",
    "print(f\"Content tokens: {len(content_oc)} (reverted {len(reverted_content_oc)})\")\n",
    "print(f\"Removed tokens: {len(removed_oc)} (reverted {len(reverted_removed_oc)})\")\n",
    "print(f\"Inserted tokens: {len(inserted_oc)} (reverted {len(reverted_inserted_oc)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content tokens: 8143191 (reverted 2910542)\n",
      "Removed tokens: 499315 (reverted 227893)\n",
      "Inserted tokens: 592580 (reverted 143858)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Content tokens: {len(content_oc)} (reverted {len(reverted_content_oc)})\")\n",
    "print(f\"Removed tokens: {len(removed_oc)} (reverted {len(reverted_removed_oc)})\")\n",
    "print(f\"Inserted tokens: {len(inserted_oc)} (reverted {len(reverted_inserted_oc)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token document frequency in reverted revisions\n",
      "Article Content\n",
      "=========================================\n",
      "                    WHITESPACE      55643\n",
      "                       NEWLINE      55484\n",
      "                            [[      55441\n",
      "                            ]]      55440\n",
      "                            \n",
      "\n",
      "      55417\n",
      "                            {{      55381\n",
      "                            }}      55378\n",
      "                             :      55369\n",
      "                             ,      55354\n",
      "                             |      55264\n",
      "                             .      55251\n",
      "                            of      55232\n",
      "                           the      55224\n",
      "                            ==      55157\n",
      "\n",
      "Removals\n",
      "=========================================\n",
      "                   REMOVAL_END      38050\n",
      "                 REMOVAL_START      38050\n",
      "                    WHITESPACE      22230\n",
      "                             |      10171\n",
      "                            ]]       9705\n",
      "                            [[       9481\n",
      "                             ,       8789\n",
      "                             .       8393\n",
      "                       NEWLINE       8185\n",
      "                           the       6767\n",
      "                            of       6514\n",
      "                             -       6323\n",
      "                           and       6245\n",
      "                             =       6075\n",
      "\n",
      "Insertions\n",
      "=========================================\n",
      "                 INSERTION_END      49110\n",
      "               INSERTION_START      49110\n",
      "                    WHITESPACE      32870\n",
      "                       NEWLINE      10544\n",
      "                             .      10366\n",
      "                             ,      10121\n",
      "                            ]]       9955\n",
      "                            [[       9781\n",
      "                             |       9631\n",
      "                           the       7840\n",
      "                           and       6989\n",
      "                            of       6979\n",
      "                             -       6618\n",
      "                            in       5779\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print some summary statistics\n",
    "print(\"Token document frequency in reverted revisions\")\n",
    "for counter_name, counter in zip(['Article Content', 'Removals', 'Insertions'], [reverted_content_oc, reverted_removed_oc, reverted_inserted_oc]):\n",
    "    print(counter_name)\n",
    "    print('='*41)\n",
    "    for token, count in counter.most_common(14):\n",
    "        if token == '\\n':\n",
    "            token = 'NEWLINE'\n",
    "        elif token == ' ':\n",
    "            token = 'WHITESPACE'\n",
    "        print(f\"{token:>30} {count:>10}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_token_odds_ratios(total_oc, reverted_oc, n=10000, min_freq=5):\n",
    "    token_odds_ratio_list = []\n",
    "    total_all_tokens_count = sum(total_oc.values())\n",
    "    reverted_all_tokens_count = sum(reverted_oc.values())\n",
    "    considered_tokens_count = 0\n",
    "    for token, total_count in tqdm(total_oc.most_common(n)):\n",
    "        if total_count < min_freq:\n",
    "            break\n",
    "        considered_tokens_count += 1\n",
    "        reverted_count = reverted_oc[token] if token in reverted_oc else 0\n",
    "        nonreverted_count = total_count - reverted_count\n",
    "        otherToken_nonreverted_count = (total_all_tokens_count - reverted_all_tokens_count) - nonreverted_count\n",
    "        otherToken_reverted_count = reverted_all_tokens_count - reverted_count\n",
    "        \n",
    "        if nonreverted_count == 0:\n",
    "            odds_ratio = 999\n",
    "        else:\n",
    "            odds_ratio = (reverted_count * otherToken_nonreverted_count) / (otherToken_reverted_count * nonreverted_count)\n",
    "        token_odds_ratio_list.append((token, odds_ratio, reverted_count, total_count))\n",
    "    if considered_tokens_count != n:\n",
    "        print(f\"Due to minimum frequency threshold, considered only {considered_tokens_count} / {n} top tokens (total unique: {len(total_oc)}).\")\n",
    "    token_odds_ratio_list.sort(key=lambda tup: tup[1], reverse=True)\n",
    "    return token_odds_ratio_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 285876.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article Content\n",
      "=========================================\n",
      "                           pc1      2.341 (1643 / 5496 = 29.89%)\n",
      "                       Hungama      2.278 (434 / 1480 = 29.32%)\n",
      "                         Chowk      2.213 (264 / 919 = 28.73%)\n",
      "                           Bhi      2.155 (290 / 1029 = 28.18%)\n",
      "                        Yamuna      2.142 (382 / 1361 = 28.07%)\n",
      "                          bcdb      2.138 (308 / 1099 = 28.03%)\n",
      "                         Taran      2.132 (278 / 994 = 27.97%)\n",
      "                         Sun's      2.116 (360 / 1294 = 27.82%)\n",
      "                     seafaring      2.110 (284 / 1023 = 27.76%)\n",
      "                     Firstpost      2.077 (480 / 1749 = 27.44%)\n",
      "                          IIFA      2.072 (286 / 1044 = 27.39%)\n",
      "                           BJP      2.069 (648 / 2368 = 27.36%)\n",
      "                         Ghazi      2.062 (563 / 2062 = 27.30%)\n",
      "                          Kher      2.059 (261 / 957 = 27.27%)\n",
      "                     Aurangzeb      2.055 (289 / 1061 = 27.24%)\n",
      "                          Rukh      2.040 (340 / 1255 = 27.09%)\n",
      "                        Adarsh      2.017 (284 / 1057 = 26.87%)\n",
      "                    suzerainty      2.014 (431 / 1606 = 26.84%)\n",
      "                         Ratna      2.012 (384 / 1432 = 26.82%)\n",
      "                        start2      2.000 (267 / 1000 = 26.70%)\n",
      "                        Bhavan      1.995 (415 / 1557 = 26.65%)\n",
      "                  timesofindia      1.991 (450 / 1691 = 26.61%)\n",
      "                        Purana      1.983 (455 / 1715 = 26.53%)\n",
      "                      Ambedkar      1.976 (353 / 1334 = 26.46%)\n",
      "                       Puranas      1.975 (245 / 926 = 26.46%)\n",
      "                      kangaroo      1.971 (309 / 1170 = 26.41%)\n",
      "                          Guha      1.967 (364 / 1380 = 26.38%)\n",
      "                      Communal      1.965 (296 / 1123 = 26.36%)\n",
      "                     Bharatiya      1.963 (767 / 2912 = 26.34%)\n",
      "                      carefree      1.948 (254 / 970 = 26.19%)\n",
      "                        Diwali      1.948 (415 / 1585 = 26.18%)\n",
      "                      Parishad      1.947 (289 / 1104 = 26.18%)\n",
      "                       Safavid      1.946 (252 / 963 = 26.17%)\n",
      "                         Lalit      1.945 (242 / 925 = 26.16%)\n",
      "                          ndtv      1.936 (275 / 1055 = 26.07%)\n",
      "                        rediff      1.934 (449 / 1724 = 26.04%)\n",
      "                        Pratap      1.934 (468 / 1797 = 26.04%)\n",
      "                      Abhishek      1.929 (422 / 1623 = 26.00%)\n",
      "                       Haryana      1.928 (772 / 2971 = 25.98%)\n",
      "                      idolatry      1.926 (248 / 955 = 25.97%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1406/50000 [00:00<00:00, 279845.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to minimum frequency threshold, considered only 1406 / 50000 top tokens (total unique: 499315).\n",
      "Removals\n",
      "=========================================\n",
      "                           Use      4.916 (528 / 796 = 66.33%)\n",
      "                       Infobox      4.905 (633 / 955 = 66.28%)\n",
      "                         dates      4.582 (538 / 831 = 64.74%)\n",
      "                       Reflist      2.987 (328 / 602 = 54.49%)\n",
      "                      External      2.633 (438 / 853 = 51.35%)\n",
      "                    References      2.567 (535 / 1055 = 50.71%)\n",
      "                       caption      2.536 (438 / 869 = 50.40%)\n",
      "                     sometimes      2.426 (352 / 714 = 49.30%)\n",
      "                      followed      2.375 (297 / 609 = 48.77%)\n",
      "                        origin      2.319 (251 / 521 = 48.18%)\n",
      "                          uses      2.317 (390 / 810 = 48.15%)\n",
      "                       changes      2.273 (246 / 516 = 47.67%)\n",
      "                    webarchive      2.253 (308 / 649 = 47.46%)\n",
      "                         Early      2.244 (313 / 661 = 47.35%)\n",
      "                         About      2.235 (369 / 781 = 47.25%)\n",
      "                        status      2.221 (243 / 516 = 47.09%)\n",
      "                         parts      2.220 (291 / 618 = 47.09%)\n",
      "                      required      2.219 (274 / 582 = 47.08%)\n",
      "                      commonly      2.210 (241 / 513 = 46.98%)\n",
      "                       reflist      2.197 (251 / 536 = 46.83%)\n",
      "                       allowed      2.179 (248 / 532 = 46.62%)\n",
      "                        nearly      2.173 (236 / 507 = 46.55%)\n",
      "                         links      2.169 (472 / 1015 = 46.50%)\n",
      "                          Many      2.166 (336 / 723 = 46.47%)\n",
      "                     generally      2.150 (287 / 620 = 46.29%)\n",
      "                    themselves      2.147 (272 / 588 = 46.26%)\n",
      "                       limited      2.123 (234 / 509 = 45.97%)\n",
      "                         terms      2.123 (268 / 583 = 45.97%)\n",
      "                          word      2.104 (280 / 612 = 45.75%)\n",
      "                         image      2.089 (726 / 1593 = 45.57%)\n",
      "                      multiple      2.084 (309 / 679 = 45.51%)\n",
      "                      referred      2.083 (273 / 600 = 45.50%)\n",
      "                        formed      2.080 (261 / 574 = 45.47%)\n",
      "                      provided      2.076 (258 / 568 = 45.42%)\n",
      "                        either      2.076 (342 / 753 = 45.42%)\n",
      "                       control      2.076 (441 / 971 = 45.42%)\n",
      "                       already      2.075 (257 / 566 = 45.41%)\n",
      "                      numerous      2.074 (227 / 500 = 45.40%)\n",
      "                         Their      2.068 (247 / 545 = 45.32%)\n",
      "                       certain      2.067 (256 / 565 = 45.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 1855/50000 [00:00<00:00, 343598.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to minimum frequency threshold, considered only 1855 / 50000 top tokens (total unique: 592580).\n",
      "Insertions\n",
      "=========================================\n",
      "                           you      5.178 (628 / 1396 = 44.99%)\n",
      "                            my      5.157 (395 / 880 = 44.89%)\n",
      "                          your      4.727 (274 / 641 = 42.75%)\n",
      "                            me      4.353 (275 / 675 = 40.74%)\n",
      "                           big      3.894 (246 / 646 = 38.08%)\n",
      "                          love      3.474 (231 / 652 = 35.43%)\n",
      "                          know      3.337 (262 / 759 = 34.52%)\n",
      "                           get      2.696 (336 / 1125 = 29.87%)\n",
      "                           got      2.672 (157 / 529 = 29.68%)\n",
      "                            we      2.566 (293 / 1016 = 28.84%)\n",
      "                          good      2.563 (300 / 1041 = 28.82%)\n",
      "                           sex      2.480 (161 / 572 = 28.15%)\n",
      "                          like      2.331 (630 / 2341 = 26.91%)\n",
      "                        famous      2.297 (168 / 631 = 26.62%)\n",
      "                          best      2.296 (408 / 1533 = 26.61%)\n",
      "                          very      2.247 (439 / 1676 = 26.19%)\n",
      "                            If      2.243 (248 / 948 = 26.16%)\n",
      "                   WHITESPACE+      2.239 (139 / 532 = 26.13%)\n",
      "                             ”      2.229 (369 / 1417 = 26.04%)\n",
      "                             >      2.224 (156 / 600 = 26.00%)\n",
      "                          it's      2.221 (153 / 589 = 25.98%)\n",
      "                            go      2.182 (253 / 987 = 25.63%)\n",
      "                           God      2.172 (140 / 548 = 25.55%)\n",
      "                             “      2.168 (352 / 1380 = 25.51%)\n",
      "                             ’      2.161 (197 / 774 = 25.45%)\n",
      "                          Also      2.156 (187 / 736 = 25.41%)\n",
      "                           And      2.092 (263 / 1059 = 24.83%)\n",
      "                          ever      2.077 (228 / 923 = 24.70%)\n",
      "                             i      2.058 (289 / 1178 = 24.53%)\n",
      "                           say      2.048 (142 / 581 = 24.44%)\n",
      "                         going      2.026 (153 / 631 = 24.25%)\n",
      "                           our      2.022 (168 / 694 = 24.21%)\n",
      "                        things      2.017 (123 / 509 = 24.17%)\n",
      "                          here      2.002 (192 / 799 = 24.03%)\n",
      "                         never      1.997 (265 / 1105 = 23.98%)\n",
      "                        battle      1.996 (128 / 534 = 23.97%)\n",
      "                        should      1.980 (279 / 1171 = 23.83%)\n",
      "                         movie      1.970 (127 / 535 = 23.74%)\n",
      "                          will      1.963 (667 / 2818 = 23.67%)\n",
      "                        killed      1.957 (205 / 868 = 23.62%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for counter_name, total_oc, reverted_oc in zip(['Article Content', 'Removals', 'Insertions'], [content_oc, removed_oc, inserted_oc], [reverted_content_oc, reverted_removed_oc, reverted_inserted_oc]):\n",
    "    token_odds_ratio_list = compute_token_odds_ratios(total_oc, reverted_oc, n=50000, min_freq=500)\n",
    "    print(counter_name)\n",
    "    print('='*41)\n",
    "    for tup in token_odds_ratio_list[:40]:\n",
    "        token, odds_ratio, reverted_count, total_count = tup\n",
    "        if token == '\\n':\n",
    "            token = 'NEWLINE'\n",
    "        elif token == ' ':\n",
    "            token = 'WHITESPACE'\n",
    "        elif token.isspace():\n",
    "            token = 'WHITESPACE+'\n",
    "        print(f\"{token:>30} {odds_ratio:>10.3f} ({reverted_count} / {total_count} = {reverted_count / total_count*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating word counts: 100%|██████████| 50000/50000 [03:27<00:00, 241.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2857456, 147161, 162179)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_counter = Counter()\n",
    "removed_counter = Counter()\n",
    "inserted_counter = Counter()\n",
    "\n",
    "include_bigrams = False\n",
    "def get_bigrams(token_list):\n",
    "    ts = token_list\n",
    "    return [ts[i] + \"_\" + ts[i+1] for i in range(len(ts) - 1)]\n",
    "\n",
    "for diff in tqdm(diff_list, desc='Generating word counts'):\n",
    "    content_counter.update(diff['content_tokens'])\n",
    "    removed_counter.update(diff['removed_tokens'])\n",
    "    inserted_counter.update(diff['inserted_tokens'])\n",
    "    if include_bigrams:\n",
    "        content_counter.update(get_bigrams(diff['content_tokens']))\n",
    "        removed_counter.update(get_bigrams(diff['removed_tokens']))\n",
    "        inserted_counter.update(get_bigrams(diff['inserted_tokens']))\n",
    "\n",
    "len(content_counter), len(removed_counter), len(inserted_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article Content\n",
      "=========================================\n",
      "                    WHITESPACE  359758534\n",
      "                             |   47220198\n",
      "                             =   26892684\n",
      "                             ,   23929390\n",
      "                             .   19249065\n",
      "                            ]]   18765321\n",
      "                            [[   18764881\n",
      "                           the   17144852\n",
      "                       NEWLINE   15312317\n",
      "                            of   11832337\n",
      "                             -   10752339\n",
      "                           and    8849511\n",
      "                            }}    7346042\n",
      "                            {{    7339362\n",
      "                            in    7090965\n",
      "                            ''    6925157\n",
      "                             \"    6305490\n",
      "                            to    5720933\n",
      "                             )    5693251\n",
      "                             (    5690791\n",
      "\n",
      "Removals\n",
      "=========================================\n",
      "                    WHITESPACE    1775020\n",
      "                             |     223926\n",
      "                 REMOVAL_START     156144\n",
      "                   REMOVAL_END     156144\n",
      "                             =     123721\n",
      "                             ,     119106\n",
      "                             .      98204\n",
      "                            ]]      86008\n",
      "                            [[      85374\n",
      "                           the      81740\n",
      "                       NEWLINE      77192\n",
      "                            of      58897\n",
      "                             -      55355\n",
      "                           and      45852\n",
      "                            ''      33156\n",
      "                             \"      32532\n",
      "                            in      32347\n",
      "                            {{      32195\n",
      "                            }}      32059\n",
      "                             )      29616\n",
      "\n",
      "Insertions\n",
      "=========================================\n",
      "                    WHITESPACE    2093132\n",
      "                             |     286010\n",
      "               INSERTION_START     171586\n",
      "                 INSERTION_END     171586\n",
      "                             =     166119\n",
      "                             ,     135141\n",
      "                             .     124823\n",
      "                            ]]     102791\n",
      "                            [[     102122\n",
      "                           the      93942\n",
      "                       NEWLINE      89926\n",
      "                             -      76199\n",
      "                            of      69620\n",
      "                           and      47987\n",
      "                            {{      44672\n",
      "                            }}      44556\n",
      "                            in      38826\n",
      "                             \"      38218\n",
      "                            ''      37220\n",
      "                             (      33120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print some summary statistics\n",
    "for counter_name, counter in zip(['Article Content', 'Removals', 'Insertions'], [content_counter, removed_counter, inserted_counter]):\n",
    "    print(counter_name)\n",
    "    print('='*41)\n",
    "    for token, count in counter.most_common(20):\n",
    "        if token == '\\n':\n",
    "            token = 'NEWLINE'\n",
    "        elif token == ' ':\n",
    "            token = 'WHITESPACE'\n",
    "        print(f\"{token:>30} {count:>10}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_vocabulary = [token for token, count in content_counter.most_common(20000)]\n",
    "removed_vocabulary = [token for token, count in content_counter.most_common(10000)]\n",
    "inserted_vocabulary = [token for token, count in content_counter.most_common(10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 10000, 10000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content_vocabulary), len(removed_vocabulary), len(inserted_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the vocabulary on all of the text documents\n",
    "# this should only include TRAINING documents, not TESTING documents\n",
    "# for now, it seems mostly innocent to compute the vocab from all documents\n",
    "\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "def get_count_vectorizer(vocabulary):\n",
    "    vectorizer = CountVectorizer(\n",
    "        tokenizer=dummy,\n",
    "        preprocessor=dummy,\n",
    "        vocabulary=vocabulary\n",
    "    )\n",
    "    return vectorizer\n",
    "\n",
    "def stream_dict_key(diffs, key):\n",
    "    for diff in diffs:\n",
    "        yield diff[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built CountVectorizer for full-page tokens in 0:05:04.321360\n"
     ]
    }
   ],
   "source": [
    "s = datetime.now()\n",
    "content_vectorizer = get_count_vectorizer(content_vocabulary)\n",
    "X_content = content_vectorizer.fit_transform(stream_dict_key(diff_list, 'content_tokens'))\n",
    "print(f\"Built CountVectorizer for full-page tokens in {datetime.now() - s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built CountVectorizer for removed tokens in 0:00:02.062464\n",
      "Built CountVectorizer for inserted tokens in 0:00:02.311446\n"
     ]
    }
   ],
   "source": [
    "s = datetime.now()\n",
    "removed_vectorizer = get_count_vectorizer(removed_vocabulary)\n",
    "X_removed = removed_vectorizer.fit_transform(stream_dict_key(diff_list, 'removed_tokens'))\n",
    "print(f\"Built CountVectorizer for removed tokens in {datetime.now() - s}\")\n",
    "\n",
    "s = datetime.now()\n",
    "inserted_vectorizer = get_count_vectorizer(inserted_vocabulary)\n",
    "X_inserted = inserted_vectorizer.fit_transform(stream_dict_key(diff_list, 'inserted_tokens'))\n",
    "print(f\"Built CountVectorizer for inserted tokens in {datetime.now() - s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 20000), (50000, 10000), (50000, 10000))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_content.shape, X_removed.shape, X_inserted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 40000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = scipy.sparse.hstack((X_content, X_removed, X_inserted))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([diff['is_reverted'] for diff in diff_list])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18394"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percentage reverted in this sample\n",
    "np.sum(y) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 3325.17, NNZs: 39215, Bias: -2.540590, T: 40000, Avg. loss: 1768.632292\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1943.07, NNZs: 39567, Bias: -2.668044, T: 80000, Avg. loss: 330.334960\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1435.10, NNZs: 39682, Bias: -2.703326, T: 120000, Avg. loss: 166.109555\n",
      "Total training time: 0.84 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1161.72, NNZs: 39764, Bias: -2.717208, T: 160000, Avg. loss: 111.601127\n",
      "Total training time: 1.11 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 981.92, NNZs: 39795, Bias: -2.731896, T: 200000, Avg. loss: 80.941698\n",
      "Total training time: 1.41 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 849.42, NNZs: 39816, Bias: -2.724177, T: 240000, Avg. loss: 60.869392\n",
      "Total training time: 1.76 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 754.45, NNZs: 39828, Bias: -2.728652, T: 280000, Avg. loss: 49.893752\n",
      "Total training time: 2.17 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 681.54, NNZs: 39838, Bias: -2.728860, T: 320000, Avg. loss: 41.535114\n",
      "Total training time: 2.53 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 623.52, NNZs: 39851, Bias: -2.738384, T: 360000, Avg. loss: 35.424810\n",
      "Total training time: 2.83 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 579.42, NNZs: 39859, Bias: -2.748498, T: 400000, Avg. loss: 32.187906\n",
      "Total training time: 3.15 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 539.68, NNZs: 39863, Bias: -2.748378, T: 440000, Avg. loss: 27.546866\n",
      "Total training time: 3.49 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 506.34, NNZs: 39869, Bias: -2.745998, T: 480000, Avg. loss: 23.846873\n",
      "Total training time: 3.81 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 477.50, NNZs: 39873, Bias: -2.748881, T: 520000, Avg. loss: 21.939090\n",
      "Total training time: 4.16 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 452.70, NNZs: 39878, Bias: -2.753595, T: 560000, Avg. loss: 19.844675\n",
      "Total training time: 4.53 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 432.18, NNZs: 39887, Bias: -2.756091, T: 600000, Avg. loss: 17.956405\n",
      "Total training time: 4.90 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 413.49, NNZs: 39889, Bias: -2.753638, T: 640000, Avg. loss: 17.341944\n",
      "Total training time: 5.21 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 396.09, NNZs: 39891, Bias: -2.755219, T: 680000, Avg. loss: 15.705670\n",
      "Total training time: 5.56 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 380.24, NNZs: 39891, Bias: -2.754020, T: 720000, Avg. loss: 14.806915\n",
      "Total training time: 5.90 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 366.62, NNZs: 39892, Bias: -2.758099, T: 760000, Avg. loss: 13.678419\n",
      "Total training time: 6.32 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 354.01, NNZs: 39893, Bias: -2.759172, T: 800000, Avg. loss: 12.722898\n",
      "Total training time: 6.65 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 342.34, NNZs: 39894, Bias: -2.758868, T: 840000, Avg. loss: 12.102274\n",
      "Total training time: 6.97 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 331.69, NNZs: 39895, Bias: -2.758751, T: 880000, Avg. loss: 11.323296\n",
      "Total training time: 7.33 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 322.20, NNZs: 39896, Bias: -2.761572, T: 920000, Avg. loss: 10.978640\n",
      "Total training time: 7.65 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 313.31, NNZs: 39897, Bias: -2.760788, T: 960000, Avg. loss: 10.323594\n",
      "Total training time: 8.00 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 304.61, NNZs: 39903, Bias: -2.760805, T: 1000000, Avg. loss: 9.683720\n",
      "Total training time: 8.37 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 297.04, NNZs: 39904, Bias: -2.762508, T: 1040000, Avg. loss: 9.235017\n",
      "Total training time: 8.69 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 289.20, NNZs: 39905, Bias: -2.761443, T: 1080000, Avg. loss: 8.850867\n",
      "Total training time: 9.05 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 282.47, NNZs: 39906, Bias: -2.762075, T: 1120000, Avg. loss: 8.292289\n",
      "Total training time: 9.42 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 275.72, NNZs: 39909, Bias: -2.761251, T: 1160000, Avg. loss: 7.749847\n",
      "Total training time: 9.82 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 269.52, NNZs: 39910, Bias: -2.760865, T: 1200000, Avg. loss: 7.605233\n",
      "Total training time: 10.19 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 263.55, NNZs: 39911, Bias: -2.761156, T: 1240000, Avg. loss: 7.393429\n",
      "Total training time: 10.54 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 258.59, NNZs: 39915, Bias: -2.762102, T: 1280000, Avg. loss: 6.732996\n",
      "Total training time: 10.88 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 254.26, NNZs: 39916, Bias: -2.762658, T: 1320000, Avg. loss: 7.003042\n",
      "Total training time: 11.22 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 249.09, NNZs: 39916, Bias: -2.764718, T: 1360000, Avg. loss: 6.433348\n",
      "Total training time: 11.57 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 244.32, NNZs: 39916, Bias: -2.763367, T: 1400000, Avg. loss: 6.191325\n",
      "Total training time: 11.93 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 239.79, NNZs: 39916, Bias: -2.762314, T: 1440000, Avg. loss: 6.076202\n",
      "Total training time: 12.27 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 235.45, NNZs: 39917, Bias: -2.761837, T: 1480000, Avg. loss: 5.974973\n",
      "Total training time: 12.62 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 231.28, NNZs: 39917, Bias: -2.761374, T: 1520000, Avg. loss: 5.752228\n",
      "Total training time: 13.00 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 227.33, NNZs: 39918, Bias: -2.763838, T: 1560000, Avg. loss: 5.450816\n",
      "Total training time: 13.32 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 223.68, NNZs: 39918, Bias: -2.762717, T: 1600000, Avg. loss: 5.380160\n",
      "Total training time: 13.62 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 219.96, NNZs: 39919, Bias: -2.764525, T: 1640000, Avg. loss: 5.066400\n",
      "Total training time: 13.96 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 216.58, NNZs: 39919, Bias: -2.764445, T: 1680000, Avg. loss: 4.950636\n",
      "Total training time: 14.26 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 213.36, NNZs: 39919, Bias: -2.762804, T: 1720000, Avg. loss: 4.779206\n",
      "Total training time: 14.56 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 210.09, NNZs: 39919, Bias: -2.764891, T: 1760000, Avg. loss: 4.645552\n",
      "Total training time: 14.86 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 207.03, NNZs: 39920, Bias: -2.763836, T: 1800000, Avg. loss: 4.652786\n",
      "Total training time: 15.20 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 204.06, NNZs: 39920, Bias: -2.765500, T: 1840000, Avg. loss: 4.477911\n",
      "Total training time: 15.56 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 201.13, NNZs: 39920, Bias: -2.765325, T: 1880000, Avg. loss: 4.526985\n",
      "Total training time: 15.87 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 198.47, NNZs: 39920, Bias: -2.764339, T: 1920000, Avg. loss: 4.302073\n",
      "Total training time: 16.19 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 196.00, NNZs: 39920, Bias: -2.764902, T: 1960000, Avg. loss: 4.118691\n",
      "Total training time: 16.50 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 193.40, NNZs: 39920, Bias: -2.765226, T: 2000000, Avg. loss: 4.070682\n",
      "Total training time: 16.90 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 190.86, NNZs: 39920, Bias: -2.763394, T: 2040000, Avg. loss: 3.901015\n",
      "Total training time: 17.20 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 188.51, NNZs: 39921, Bias: -2.765119, T: 2080000, Avg. loss: 3.839174\n",
      "Total training time: 17.57 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 186.25, NNZs: 39921, Bias: -2.763894, T: 2120000, Avg. loss: 3.766000\n",
      "Total training time: 17.88 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 183.94, NNZs: 39922, Bias: -2.763625, T: 2160000, Avg. loss: 3.599723\n",
      "Total training time: 18.18 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 181.75, NNZs: 39922, Bias: -2.763712, T: 2200000, Avg. loss: 3.549849\n",
      "Total training time: 18.64 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 179.62, NNZs: 39924, Bias: -2.764773, T: 2240000, Avg. loss: 3.662166\n",
      "Total training time: 19.17 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 177.67, NNZs: 39924, Bias: -2.763169, T: 2280000, Avg. loss: 3.433309\n",
      "Total training time: 19.58 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 175.70, NNZs: 39925, Bias: -2.764428, T: 2320000, Avg. loss: 3.364243\n",
      "Total training time: 19.91 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 173.83, NNZs: 39925, Bias: -2.763996, T: 2360000, Avg. loss: 3.355024\n",
      "Total training time: 20.22 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 171.93, NNZs: 39926, Bias: -2.763841, T: 2400000, Avg. loss: 3.165153\n",
      "Total training time: 20.59 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 170.05, NNZs: 39927, Bias: -2.764260, T: 2440000, Avg. loss: 3.048745\n",
      "Total training time: 20.90 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 168.30, NNZs: 39927, Bias: -2.764737, T: 2480000, Avg. loss: 3.028112\n",
      "Total training time: 21.25 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 166.56, NNZs: 39927, Bias: -2.763631, T: 2520000, Avg. loss: 3.072630\n",
      "Total training time: 21.57 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 164.86, NNZs: 39927, Bias: -2.763173, T: 2560000, Avg. loss: 2.976443\n",
      "Total training time: 21.86 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 163.17, NNZs: 39927, Bias: -2.764807, T: 2600000, Avg. loss: 2.891591\n",
      "Total training time: 22.20 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 161.53, NNZs: 39929, Bias: -2.764357, T: 2640000, Avg. loss: 2.840035\n",
      "Total training time: 22.55 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 159.90, NNZs: 39930, Bias: -2.763950, T: 2680000, Avg. loss: 2.709692\n",
      "Total training time: 22.89 seconds.\n",
      "-- Epoch 68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 158.32, NNZs: 39930, Bias: -2.763552, T: 2720000, Avg. loss: 2.758155\n",
      "Total training time: 23.23 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 156.76, NNZs: 39930, Bias: -2.763567, T: 2760000, Avg. loss: 2.731135\n",
      "Total training time: 23.52 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 155.31, NNZs: 39931, Bias: -2.763865, T: 2800000, Avg. loss: 2.630099\n",
      "Total training time: 23.83 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 153.78, NNZs: 39931, Bias: -2.763297, T: 2840000, Avg. loss: 2.679160\n",
      "Total training time: 24.19 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 152.36, NNZs: 39931, Bias: -2.763759, T: 2880000, Avg. loss: 2.547628\n",
      "Total training time: 24.54 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 150.98, NNZs: 39931, Bias: -2.763252, T: 2920000, Avg. loss: 2.580961\n",
      "Total training time: 24.86 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 149.71, NNZs: 39931, Bias: -2.763464, T: 2960000, Avg. loss: 2.477265\n",
      "Total training time: 25.15 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 148.49, NNZs: 39932, Bias: -2.764203, T: 3000000, Avg. loss: 2.442070\n",
      "Total training time: 25.45 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 147.34, NNZs: 39932, Bias: -2.763136, T: 3040000, Avg. loss: 2.342447\n",
      "Total training time: 25.78 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 146.15, NNZs: 39932, Bias: -2.763050, T: 3080000, Avg. loss: 2.339356\n",
      "Total training time: 26.13 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 144.88, NNZs: 39932, Bias: -2.763765, T: 3120000, Avg. loss: 2.271676\n",
      "Total training time: 26.49 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 143.70, NNZs: 39932, Bias: -2.763846, T: 3160000, Avg. loss: 2.267075\n",
      "Total training time: 26.81 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 142.53, NNZs: 39932, Bias: -2.764033, T: 3200000, Avg. loss: 2.257449\n",
      "Total training time: 27.09 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 141.40, NNZs: 39932, Bias: -2.763219, T: 3240000, Avg. loss: 2.193159\n",
      "Total training time: 27.42 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 140.25, NNZs: 39932, Bias: -2.764113, T: 3280000, Avg. loss: 2.103762\n",
      "Total training time: 27.72 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 139.14, NNZs: 39932, Bias: -2.763963, T: 3320000, Avg. loss: 2.071110\n",
      "Total training time: 28.01 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 138.06, NNZs: 39932, Bias: -2.763307, T: 3360000, Avg. loss: 2.032805\n",
      "Total training time: 28.33 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 137.05, NNZs: 39932, Bias: -2.763458, T: 3400000, Avg. loss: 2.039342\n",
      "Total training time: 28.65 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 136.02, NNZs: 39932, Bias: -2.763386, T: 3440000, Avg. loss: 2.031867\n",
      "Total training time: 28.97 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 134.94, NNZs: 39932, Bias: -2.763490, T: 3480000, Avg. loss: 2.018985\n",
      "Total training time: 29.30 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 133.92, NNZs: 39932, Bias: -2.763815, T: 3520000, Avg. loss: 2.013115\n",
      "Total training time: 29.59 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 132.93, NNZs: 39932, Bias: -2.763448, T: 3560000, Avg. loss: 1.949839\n",
      "Total training time: 29.89 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 131.98, NNZs: 39932, Bias: -2.763781, T: 3600000, Avg. loss: 1.870619\n",
      "Total training time: 30.23 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 131.05, NNZs: 39932, Bias: -2.762818, T: 3640000, Avg. loss: 1.864763\n",
      "Total training time: 30.56 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 130.16, NNZs: 39938, Bias: -2.763497, T: 3680000, Avg. loss: 1.904602\n",
      "Total training time: 30.91 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 129.19, NNZs: 39938, Bias: -2.763648, T: 3720000, Avg. loss: 1.835702\n",
      "Total training time: 31.21 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 128.27, NNZs: 39938, Bias: -2.763463, T: 3760000, Avg. loss: 1.797423\n",
      "Total training time: 31.54 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 127.39, NNZs: 39938, Bias: -2.763302, T: 3800000, Avg. loss: 1.771971\n",
      "Total training time: 31.90 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 126.52, NNZs: 39938, Bias: -2.763817, T: 3840000, Avg. loss: 1.721029\n",
      "Total training time: 32.22 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 125.70, NNZs: 39938, Bias: -2.763509, T: 3880000, Avg. loss: 1.767138\n",
      "Total training time: 32.57 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 124.91, NNZs: 39938, Bias: -2.763699, T: 3920000, Avg. loss: 1.733416\n",
      "Total training time: 32.90 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 124.07, NNZs: 39938, Bias: -2.763315, T: 3960000, Avg. loss: 1.710029\n",
      "Total training time: 33.23 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 123.27, NNZs: 39938, Bias: -2.763817, T: 4000000, Avg. loss: 1.649944\n",
      "Total training time: 33.53 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 122.50, NNZs: 39938, Bias: -2.763186, T: 4040000, Avg. loss: 1.648048\n",
      "Total training time: 33.88 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 121.72, NNZs: 39938, Bias: -2.763944, T: 4080000, Avg. loss: 1.697900\n",
      "Total training time: 34.20 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 120.94, NNZs: 39938, Bias: -2.763218, T: 4120000, Avg. loss: 1.603545\n",
      "Total training time: 34.53 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 120.18, NNZs: 39938, Bias: -2.763618, T: 4160000, Avg. loss: 1.541867\n",
      "Total training time: 34.85 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 119.42, NNZs: 39938, Bias: -2.762788, T: 4200000, Avg. loss: 1.582766\n",
      "Total training time: 35.20 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 118.67, NNZs: 39938, Bias: -2.763454, T: 4240000, Avg. loss: 1.578221\n",
      "Total training time: 35.55 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 117.92, NNZs: 39938, Bias: -2.763341, T: 4280000, Avg. loss: 1.457899\n",
      "Total training time: 35.87 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 117.19, NNZs: 39938, Bias: -2.763476, T: 4320000, Avg. loss: 1.525848\n",
      "Total training time: 36.18 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 116.58, NNZs: 39938, Bias: -2.763566, T: 4360000, Avg. loss: 1.486906\n",
      "Total training time: 36.50 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 115.93, NNZs: 39938, Bias: -2.764014, T: 4400000, Avg. loss: 1.470381\n",
      "Total training time: 36.82 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 115.23, NNZs: 39938, Bias: -2.763703, T: 4440000, Avg. loss: 1.475275\n",
      "Total training time: 37.10 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 114.56, NNZs: 39938, Bias: -2.763105, T: 4480000, Avg. loss: 1.463030\n",
      "Total training time: 37.42 seconds.\n",
      "Convergence after 112 epochs took 37.42 seconds\n",
      "0:01:11.195617\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.20, random_state=500)\n",
    "s = datetime.now()\n",
    "\n",
    "clf = sklearn.linear_model.LogisticRegression(\n",
    "    penalty='l2',\n",
    "    C=1.0,\n",
    "    solver='lbfgs',\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "#clf = sklearn.svm.LinearSVC(\n",
    "#    C=0.1,\n",
    "#    dual=False,\n",
    "#)\n",
    "clf = sklearn.linear_model.SGDClassifier(\n",
    "    loss='log',\n",
    "    penalty='l2',\n",
    "    early_stopping=False,\n",
    "    validation_fraction=0.05,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# scaling can help some of the solvers converge more rapidly...\n",
    "X_train = sklearn.preprocessing.scale(X_train, with_mean=False)\n",
    "X_test = sklearn.preprocessing.scale(X_test, with_mean=False)\n",
    "\n",
    "# train the model\n",
    "md = clf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"{datetime.now() - s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1786, 0.7693, 0.6843365312510097)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test = md.predict(X_test)\n",
    "y_pred_test_proba = md.predict_proba(X_test)[:,1]\n",
    "#y_pred_test_proba = 1 / (1 + np.exp(-md.decision_function(X_test))) # can use as lazy eval for models without a probability output\n",
    "\n",
    "pct_predicted_reverted = np.sum(y_pred_test) / len(y_pred_test)\n",
    "test_acc = np.sum(y_test == y_pred_test) / len(y_test)\n",
    "roc_auc = sklearn.metrics.roc_auc_score(y_test, y_pred_test_proba)\n",
    "pct_predicted_reverted, test_acc, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 40000)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_token_weights = list(zip(content_vocabulary, clf.coef_[0,:20000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_token_weights = list(zip(removed_vocabulary, clf.coef_[0,20000:30000]))\n",
    "inserted_token_weights = list(zip(inserted_vocabulary, clf.coef_[0,30000:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_token_weights.sort(key=lambda tup: abs(tup[1]), reverse=True)\n",
    "removed_token_weights.sort(key=lambda tup: abs(tup[1]), reverse=True)\n",
    "inserted_token_weights.sort(key=lambda tup: abs(tup[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                SAGE   -2.964\n",
      "                  Hi   1.875\n",
      "                 725   1.853\n",
      "    increaseNegative   -1.828\n",
      "          References   -1.806\n",
      "              emerge   1.760\n",
      "                 443   -1.754\n",
      "                 431   -1.720\n",
      "           Frederick   1.710\n",
      "           Messenger   -1.708\n",
      "        dictatorship   1.688\n",
      "         approximate   1.674\n",
      "           frameless   1.670\n",
      "                  cn   -1.635\n",
      "                 pc1   1.629\n",
      "                 618   1.589\n",
      "      organizational   -1.589\n",
      "           spokesman   -1.567\n",
      "               Alive   1.566\n",
      "            respects   -1.560\n",
      "         subdivision   1.553\n",
      "              select   1.540\n",
      "                  ==   -1.535\n",
      "                 224   -1.516\n",
      "            sciences   1.512\n",
      "           <nowiki/>   -1.510\n",
      "             amazing   1.508\n",
      "          attributed   -1.505\n",
      "              Alonso   1.490\n",
      "              image4   -1.484\n"
     ]
    }
   ],
   "source": [
    "for token, weight in content_token_weights[:30]:\n",
    "    print(f\"{token:>20}   {weight:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                2018   -5.142\n",
      "                hold   3.699\n",
      "        Publications   3.652\n",
      "           Minnesota   3.641\n",
      "                  me   -3.596\n",
      "                wide   3.592\n",
      "           sovereign   3.480\n",
      "                 202   3.441\n",
      "                  Is   -3.427\n",
      "                  69   -3.423\n",
      "                   ‘   -3.377\n",
      "                 480   3.353\n",
      "                  MF   -3.273\n",
      "                soil   -3.129\n",
      "                Fact   3.127\n",
      "           Microsoft   3.108\n",
      "            Examples   3.105\n",
      "             article   -3.024\n",
      "                  32   3.021\n",
      "                1920   3.009\n",
      "                  63   2.991\n",
      "               until   2.970\n",
      "                  SS   -2.963\n",
      "             India's   -2.960\n",
      "                 GER   -2.959\n",
      "              causes   2.920\n",
      "                   '   -2.880\n",
      "                1952   2.854\n",
      "            reliable   -2.803\n",
      "                   [   -2.793\n",
      "             cropped   2.774\n",
      "                  43   2.763\n",
      "            greatest   -2.754\n",
      "               elite   -2.733\n",
      "           Americans   2.704\n",
      "             Infobox   2.694\n",
      "              occurs   -2.687\n",
      "                  my   -2.681\n",
      "               Track   -2.642\n",
      "              Berber   2.635\n",
      "              Chiefs   -2.633\n",
      "                 you   -2.630\n",
      "           amendment   -2.618\n",
      "                 PhD   -2.606\n",
      "                Holy   -2.593\n",
      "             Richard   2.590\n",
      "            replaced   2.572\n",
      "               Jammu   2.541\n",
      "               Hindi   -2.530\n",
      "        universities   2.512\n"
     ]
    }
   ],
   "source": [
    "for token, weight in removed_token_weights[:50]:\n",
    "    print(f\"{token:>20}   {weight:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           platforms   -9.751\n",
      "                  my   6.570\n",
      "           Meanwhile   -4.333\n",
      "                 big   4.331\n",
      "              decade   -3.890\n",
      "                 Jew   3.760\n",
      "                 978   -3.692\n",
      "                 209   3.676\n",
      "                Jeff   3.673\n",
      "           suspected   -3.648\n",
      "               elite   3.531\n",
      "             website   -3.498\n",
      "                very   3.484\n",
      "                  Is   3.415\n",
      "              little   3.289\n",
      "                 Use   -3.261\n",
      "           practiced   3.200\n",
      "                 124   3.195\n",
      "                  IT   3.166\n",
      "                  DF   -3.090\n",
      "            duration   3.086\n",
      "               rifle   -3.078\n",
      "       controversial   3.072\n",
      "               jstor   -3.031\n",
      "           September   -2.940\n",
      "              linear   2.928\n",
      "          Portuguese   -2.923\n",
      "              killed   2.922\n",
      "                 you   2.921\n",
      "             present   -2.911\n",
      "             because   2.890\n",
      "            affected   2.878\n",
      "             archive   -2.874\n",
      "             Opening   -2.862\n",
      "             gallery   -2.843\n",
      "                       2.827\n",
      "              Ulster   -2.820\n",
      "                  df   -2.816\n",
      "          introduced   -2.809\n",
      "              Chiang   -2.798\n",
      "            Hispanic   2.791\n",
      "             hunting   -2.749\n",
      "           fertility   -2.743\n",
      "                 alt   -2.736\n",
      "             alcohol   -2.704\n",
      "                dead   -2.694\n",
      "                save   -2.680\n",
      "          immigrants   2.675\n",
      "                 gay   2.663\n",
      "      disambiguation   -2.655\n"
     ]
    }
   ],
   "source": [
    "for token, weight in inserted_token_weights[:50]:\n",
    "    print(f\"{token:>20}   {weight:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Flagon Python3",
   "language": "python",
   "name": "flagon-conda-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
