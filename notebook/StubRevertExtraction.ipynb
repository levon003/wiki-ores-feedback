{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revert Extraction\n",
    "===\n",
    "\n",
    "Goal: Extract revisions from the stub-history files.\n",
    "\n",
    "These files contain sha1's for the text of revisions, which I use to identify reverted revisions.\n",
    "\n",
    "Every _revision_ can be either:\n",
    " - A revert revision\n",
    " - A reverted revision\n",
    " - A regular revision (and revert target)\n",
    " - A regular revision (and non-revert target)\n",
    " \n",
    "For now, I think I'll just save all reverts....\n",
    "\n",
    "...and perhaps subsequently all revisions? We'll see.\n",
    "\n",
    "Target daterange: Jan 01, 2018 - Jan 01, 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mwapi\n",
    "import mwxml\n",
    "import mwxml.utilities\n",
    "import mwcli\n",
    "import mwreverts\n",
    "import oresapi\n",
    "import mwparserfromhell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import bz2\n",
    "import gzip\n",
    "import json\n",
    "import re\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "import scipy.stats\n",
    "import para\n",
    "from itertools import groupby\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/export/scratch2/levon003/repos/wiki-ores-feedback'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "git_root_dir = !git rev-parse --show-toplevel\n",
    "git_root_dir = git_root_dir[0]\n",
    "git_root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/export/scratch2/wiki_data',\n",
       " '/export/scratch2/levon003/repos/wiki-ores-feedback/data/derived')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_dir = \"/export/scratch2/wiki_data\"\n",
    "derived_data_dir = os.path.join(git_root_dir, \"data\", \"derived\")\n",
    "raw_data_dir, derived_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/export/scratch2/levon003/repos/wiki-ores-feedback/data/derived/stub-history-reverts'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_dir = os.path.join(derived_data_dir, 'stub-history-reverts')\n",
    "os.makedirs(working_dir, exist_ok=True)\n",
    "working_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19881980"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_index_path = os.path.join(raw_data_dir, \"enwiki-20200101-pages-articles-multistream-index.txt\")\n",
    "article_index = open(article_index_path).readlines()\n",
    "len(article_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19881980/19881980 [00:33<00:00, 595704.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19881980"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_title_dict = {}\n",
    "for line in tqdm(article_index):\n",
    "    tokens = line.strip().split(\":\")\n",
    "    #page_start_bytes = int(tokens[0])\n",
    "    page_id = int(tokens[1])\n",
    "    page_title = \"\".join(tokens[2:])\n",
    "    page_title_dict[page_id] = page_title\n",
    "len(page_title_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stub_history_dir = os.path.join(raw_data_dir, \"enwiki-20200101-stub-meta-history-gz\")\n",
    "assert os.path.exists(stub_history_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = [os.path.join(stub_history_dir, stub_history_filename) \n",
    "         for stub_history_filename in os.listdir(stub_history_dir)\n",
    "         if stub_history_filename.endswith(\".xml.gz\")]\n",
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1514786400, 1577858400)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_date = datetime.fromisoformat('2018-01-01')\n",
    "start_timestamp = int(start_date.timestamp())\n",
    "end_date = datetime.fromisoformat('2020-01-01')\n",
    "end_timestamp = int(end_date.timestamp())\n",
    "start_timestamp, end_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/mediawiki-utilities/python-mwxml/blob/master/mwxml/utilities/dump2revdocs.py\n",
    "def dump2revdocs(dump, verbose=False):\n",
    "    for page in dump:\n",
    "        if page.namespace == 0 and page.redirect is None:\n",
    "            for revision in page:\n",
    "                yield revision.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dump(dump, ndjson_filepath):\n",
    "    with open(ndjson_filepath, 'w') as outfile:\n",
    "        for page in dump:\n",
    "            if page.namespace != 0 or page.redirect is not None:\n",
    "                continue\n",
    "            page_id = page.id\n",
    "            rev_count = 0\n",
    "\n",
    "            rev_tups = []\n",
    "            is_revert_target_set = set()\n",
    "            is_reverted_set = set()\n",
    "            is_reverting_set = set()\n",
    "\n",
    "            # we use a new detector for each page\n",
    "            detector = mwreverts.Detector(radius=15)\n",
    "            for revision in page:\n",
    "                rev_count += 1\n",
    "                \n",
    "                # convert each revision to json and extract the relevant info from it\n",
    "                rev_doc = revision.to_json()\n",
    "                rev_id = rev_doc['id']\n",
    "                rev_timestamp = int(datetime.strptime(rev_doc['timestamp'], \"%Y-%m-%dT%H:%M:%SZ\").timestamp())\n",
    "                rev_tup = [page_id, rev_id, rev_timestamp]\n",
    "                rev_tups.append(rev_tup)\n",
    "\n",
    "                # now, we check if we have identified a new revert\n",
    "                checksum = rev_doc.get('sha1') or mwreverts.DummyChecksum()\n",
    "                revert = detector.process(checksum, rev_doc)\n",
    "                \n",
    "                # we only consider reverts in the target timerange\n",
    "                if revert and rev_timestamp >= start_timestamp and rev_timestamp <= end_timestamp:\n",
    "                    revert_json = revert.to_json()\n",
    "\n",
    "                    reverting_id = revert_json['reverting']['id']\n",
    "                    reverted_to_id = revert_json['reverted_to']['id']\n",
    "                    reverteds_ids = [rev['id'] for rev in revert_json['reverteds']]\n",
    "\n",
    "                    # keep track of which revision ids are reverts/reverting/reverted-to-targets\n",
    "                    is_reverting_set.add(reverting_id)\n",
    "                    is_revert_target_set.add(reverted_to_id)\n",
    "                    is_reverted_set.update(reverteds_ids)\n",
    "\n",
    "                    # we save reverts to an ndjson file\n",
    "                    outfile.write(str(revert_json) + \"\\n\")\n",
    "\n",
    "            # having processed for reverts, we output all revisions along with their types back to the central process\n",
    "            for rev_tup in rev_tups:\n",
    "                page_id, rev_id, rev_timestamp = rev_tup\n",
    "                if rev_timestamp >= start_timestamp and rev_timestamp <= end_timestamp:\n",
    "                    is_revert_target = int(rev_id in is_revert_target_set)\n",
    "                    is_reverted = int(rev_id in is_reverted_set)\n",
    "                    is_reverting = int(rev_id in is_reverting_set)\n",
    "                    yield page_id, rev_id, rev_timestamp, is_revert_target, is_reverted, is_reverting\n",
    "\n",
    "def process_stub_history_filepath(path):\n",
    "    \"\"\"\n",
    "    :path str: string path to a Gzip-ed Wikipedia XML file. Designed to be called with stub history files.\n",
    "    \"\"\"\n",
    "    with gzip.open(path, 'rt', encoding='utf-8', errors='replace') as infile:\n",
    "        dump = mwxml.Dump.from_file(infile)\n",
    "        ndjson_filename = os.path.splitext(os.path.basename(path))[0] + \"-reverts.ndjson\"\n",
    "        ndjson_filepath = os.path.join(working_dir, ndjson_filename)        \n",
    "        results = process_dump(dump, ndjson_filepath)\n",
    "        #rev_docs = dump2revdocs(dump)\n",
    "        #results = process_rev_docs(rev_docs)\n",
    "        yield from results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell demonstrates processing a single file\n",
    "start = datetime.now()\n",
    "with open(os.path.join(working_dir, 'rev_ids_single_file.csv'), 'w') as outfile:\n",
    "    for result in process_stub_history_filepath(paths[0]):\n",
    "        page_id, rev_id, rev_timestamp, is_revert_target, is_reverted, is_reverting = result\n",
    "        outfile.write(f\"{page_id},{rev_id},{rev_timestamp},{is_revert_target},{is_reverted},{is_reverting}\\n\")\n",
    "print(f\"{datetime.now() - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process all files in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:13:01.393968\n"
     ]
    }
   ],
   "source": [
    "# this cell uses para to process all of the history files in parallel\n",
    "start = datetime.now()\n",
    "with open(os.path.join(working_dir, 'rev_ids.csv'), 'w') as outfile:\n",
    "    for result in para.map(process_stub_history_filepath, paths, mappers=len(paths)):\n",
    "        page_id, rev_id, rev_timestamp, is_revert_target, is_reverted, is_reverting = result\n",
    "        outfile.write(f\"{page_id},{rev_id},{rev_timestamp},{is_revert_target},{is_reverted},{is_reverting}\\n\")\n",
    "print(f\"{datetime.now() - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial runtime was 13 hours, 13 minutes, which is quite reasonable.  The total storage required for the revision ids file and the reverts json was 13GB, which is also quite reasonable.\n",
    "\n",
    "In total, we identified 5,992,682 reverts in namespace-0 non-redirect enwiki pages from 2018-2020.\n",
    "\n",
    "We identified 77,287,697 total revisions on the same set of pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07753733430561399"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7.8% of revisions are reverts\n",
    "5992682 / 77287697"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output file is sorted using bash:\n",
    "\n",
    "```\n",
    "sort -k1 -n -t, rev_ids.csv > rev_ids_sorted.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Flagon Python3",
   "language": "python",
   "name": "flagon-conda-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
