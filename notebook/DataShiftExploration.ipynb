{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Shift Exploration\n",
    "===\n",
    "\n",
    "Mocking labeled revisions so that rev-scoring can be applied.\n",
    "\n",
    "#### Analysis plan:\n",
    "\n",
    "Basic intuition:\n",
    "\n",
    "For continuous variables, use the two-sample K-S test.\n",
    "For binary variables, use the two-sample Chi Squared.  (Could consider a G-test instead...)\n",
    "\n",
    "To compare source and target distribution, conduct a test for each variable independently.  Then, apply Bonf. correction for the multiple comparisons (Bonf. correction equal to the number of variables.)\n",
    "Then, reject the null hypothesis if the minimum p-value among all tests is less than 0.05/K (K is number of tests).\n",
    "Thus, ANY significant difference after correction is taken to be evidence of distribution shift.\n",
    "\n",
    "\n",
    "How to measure magnitude?\n",
    "This is a hard problem... Could sum the Earth Mover's Distance, perhaps?  But, would want to normalize by the range of the feature in some way.\n",
    "I need to do more reading about how to quantify the magnitude of a data shift.\n",
    "Could use multivariate KL divergence?\n",
    "\n",
    "Could also use a Multivariate test:\n",
    "Consider either Maximum Mean Discrepancy (as Lipton used) or Cramer's test: https://cran.r-project.org/web/packages/cramer/cramer.pdf\n",
    "\n",
    "Should also train a classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mwapi\n",
    "import mwxml\n",
    "import mwxml.utilities\n",
    "import mwcli\n",
    "import mwreverts\n",
    "import oresapi\n",
    "import mwparserfromhell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import bz2\n",
    "import gzip\n",
    "import json\n",
    "import re\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "import scipy.stats\n",
    "import para\n",
    "from itertools import groupby\n",
    "from collections import Counter\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/export/scratch2/levon003/repos/wiki-ores-feedback'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "git_root_dir = !git rev-parse --show-toplevel\n",
    "git_root_dir = git_root_dir[0]\n",
    "git_root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "revisions_features_filepath = os.path.join(git_root_dir, \"data/raw/editquality/datasets/enwiki.labeled_revisions.20k_2015.damaging.tsv\")\n",
    "assert os.path.exists(revisions_features_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19348"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df = pd.read_csv(revisions_features_filepath, sep='\\t', header=0)\n",
    "len(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19348"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_list = []\n",
    "revisions_with_cache_filepath = os.path.join(git_root_dir, \"data/raw/editquality/datasets/enwiki.labeled_revisions.w_cache.20k_2015.json\")\n",
    "with open(revisions_with_cache_filepath, 'r') as infile:\n",
    "    for line in infile:\n",
    "        rev = json.loads(line)\n",
    "        rev_list.append({\n",
    "            'rev_id': rev['rev_id'],\n",
    "            'damaging': rev['damaging'],\n",
    "            'goodfaith': rev['goodfaith']\n",
    "        })\n",
    "df = pd.DataFrame(rev_list)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, features_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'19344 / 19348 (99.98%) training revisions have associated content.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in the labeled revisions content data\n",
    "labeled_revs_dir = os.path.join(git_root_dir, \"data/derived/labeled-revs\")\n",
    "labeled_revs_filepath = os.path.join(labeled_revs_dir, 'labeled_revisions.20k_2015.content.ndjson')\n",
    "results = []\n",
    "with open(labeled_revs_filepath, 'r') as infile:\n",
    "    for line in infile:\n",
    "        result = json.loads(line)\n",
    "        results.append(result)\n",
    "\n",
    "#Identify revision ids for which content is available\n",
    "rev_ids_with_content = set()\n",
    "for result in results:\n",
    "    for page in result['pages']:\n",
    "        for rev in page['revisions']:\n",
    "            rev_id = rev['revid']\n",
    "            rev_ids_with_content.add(rev_id)\n",
    "df['has_content'] = df.rev_id.map(lambda rev_id: rev_id in rev_ids_with_content)\n",
    "f\"{np.sum(df.has_content)} / {len(df)} ({np.sum(df.has_content) / len(df)*100:.2f}%) training revisions have associated content.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13559"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_ids_meeting_criteria = set()\n",
    "for result in results:\n",
    "    for page in result['pages']:\n",
    "        if page['ns'] != 0 or 'redirect' in page:\n",
    "            continue\n",
    "        for rev in page['revisions']:\n",
    "            rev_id = rev['revid']\n",
    "            rev_ids_meeting_criteria.add(rev_id)\n",
    "df['meets_criteria'] = df.rev_id.map(lambda rev_id: rev_id in rev_ids_meeting_criteria)\n",
    "len(rev_ids_meeting_criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13529, 82)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df = df.loc[df.meets_criteria,features_df.columns]\n",
    "features_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sample1 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dir = \"/export/scratch2/wiki_data\"\n",
    "derived_data_dir = os.path.join(git_root_dir, \"data\", \"derived\")\n",
    "mock_features_filepath = os.path.join(git_root_dir, \"data/derived/labeled-revs/sample1.mock.damaging.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "458449"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mock_features_df = pd.read_csv(mock_features_filepath, sep='\\t', header=0)\n",
    "len(mock_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load month_sample features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_sample_features_dir = \"/export/scratch2/levon003/repos/wiki-ores-feedback/data/derived/stub-history-all-revisions/month_sample/revscoring_features\"\n",
    "month_sample_filepath = os.path.join(month_sample_features_dir, \"rev_ids_month_sample_2015_01.mock.damaging.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99789"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "month_sample_df = pd.read_csv(month_sample_filepath, sep='\\t', header=0)\n",
    "len(month_sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Univariate Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define source and target dataframes\n",
    "sdf = features_df.drop(columns='damaging')\n",
    "#tdf = month_sample_df.drop(columns='damaging')\n",
    "tdf = mock_features_df.drop(columns='damaging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13529, 80), (458449, 80))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.shape, tdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature.revision.page.is_articleish                                                   bool\n",
       "feature.revision.page.is_mainspace                                                    bool\n",
       "feature.revision.page.is_draftspace                                                   bool\n",
       "feature.log((wikitext.revision.parent.chars + 1))                                  float64\n",
       "feature.log((len(<datasource.tokenized(datasource.revision.parent.text)>) + 1))    float64\n",
       "                                                                                    ...   \n",
       "feature.english.dictionary.revision.diff.non_dict_word_delta_increase                int64\n",
       "feature.english.dictionary.revision.diff.non_dict_word_delta_decrease                int64\n",
       "feature.english.dictionary.revision.diff.non_dict_word_prop_delta_sum              float64\n",
       "feature.english.dictionary.revision.diff.non_dict_word_prop_delta_increase         float64\n",
       "feature.english.dictionary.revision.diff.non_dict_word_prop_delta_decrease         float64\n",
       "Length: 80, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/scratch2/levon003/bin/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Name                                                            Feat. Type p-value S T   Diff\n",
      "====================================================================================================\n",
      "log((wikitext.revision.parent.ref_tags + 1))                            continuous   0.000 Y Y   0.92 (2.32) 39.7%\n",
      "log((wikitext.revision.parent.external_links + 1))                      continuous   0.000 Y Y   0.87 (2.28) 38.1%\n",
      "log((wikitext.revision.parent.templates + 1))                           continuous   0.000 Y Y   0.98 (3.02) 32.4%\n",
      "log((wikitext.revision.parent.headings + 1))                            continuous   0.000 Y Y   0.47 (2.11) 22.2%\n",
      "log((wikitext.revision.parent.wikilinks + 1))                           continuous   0.000 Y Y   0.73 (4.12) 17.7%\n",
      "log((len(<datasource.wikitext.revision.parent.words>) + 1))             continuous   0.000 Y Y   0.90 (6.97) 12.9%\n",
      "log((len(<datasource.tokenized(datasource.revision.parent.text)>) + 1)) continuous   0.000 Y Y   0.93 (8.01) 11.6%\n",
      "log((wikitext.revision.parent.chars + 1))                               continuous   0.000 Y Y   0.98 (9.08) 10.7%\n",
      "log((temporal.revision.user.seconds_since_registration + 1))            continuous   0.000 Y Y  -0.47 (13.50) -3.5%\n",
      "log((len(<datasource.wikitext.revision.parent.uppercase_words>) + 1))   continuous   0.000 Y Y   0.62 (3.13) 19.7%\n",
      "revision.parent.chars_per_word                                          continuous   0.000 Y -   0.20 (8.92) 2.3%\n",
      "wikitext.revision.diff.markup_prop_delta_increase                       continuous   0.000 Y -  -0.99 (1.90) -52.0%\n",
      "wikitext.revision.diff.markup_prop_delta_sum                            continuous   0.000 Y -  -0.96 (1.82) -52.8%\n",
      "revision.parent.uppercase_words_per_word                                continuous   0.000 Y Y  -0.01 (0.03) -20.5%\n",
      "revision.parent.markups_per_token                                       continuous   0.000 Y -   0.00 (0.09) 1.2%\n",
      "english.dictionary.revision.diff.dict_word_prop_delta_increase          continuous   0.000 Y -  -2.71 (12.22) -22.1%\n",
      "english.dictionary.revision.diff.dict_word_prop_delta_sum               continuous   0.000 Y -  -2.40 (8.33) -28.8%\n",
      "revision.parent.words_per_token                                         continuous   0.000 Y Y   0.00 (0.34) 1.3%\n",
      "wikitext.revision.diff.number_delta_decrease                            continuous   0.000 Y -  -0.33 (-1.53) 21.7%\n",
      "wikitext.revision.diff.number_prop_delta_decrease                       continuous   0.000 Y -  -0.06 (-0.45) 13.7%\n",
      "revision.diff.words_change                                              continuous   0.000 Y -  -2.17 (8.93) -24.4%\n",
      "wikitext.revision.diff.number_delta_increase                            continuous   0.000 Y -   0.45 (2.50) 17.9%\n",
      "revision.diff.chars_change                                              continuous   0.000 Y - -13.86 (77.97) -17.8%\n",
      "english.dictionary.revision.diff.dict_word_delta_sum                    continuous   0.000 Y -  -2.16 (7.71) -28.1%\n",
      "revision.diff.longest_new_token                                         continuous   0.000 Y -  -0.58 (4.93) -11.8%\n",
      "wikitext.revision.diff.number_prop_delta_increase                       continuous   0.000 Y -  -0.19 (1.58) -11.8%\n",
      "revision.diff.tokens_change                                             continuous   0.000 Y -  -5.70 (26.11) -21.8%\n",
      "revision.diff.wikilinks_change                                          continuous   0.000 Y -  -0.10 (0.38) -27.3%\n",
      "wikitext.revision.diff.markup_delta_increase                            continuous   0.000 Y -   0.31 (4.92) 6.2%\n",
      "wikitext.revision.diff.markup_delta_sum                                 continuous   0.000 Y -  -0.31 (2.32) -13.2%\n",
      "revision.diff.markups_change                                            continuous   0.000 Y -  -0.31 (2.32) -13.2%\n",
      "english.dictionary.revision.diff.dict_word_delta_increase               continuous   0.000 Y -  -0.29 (19.32) -1.5%\n",
      "english.dictionary.revision.diff.non_dict_word_prop_delta_increase      continuous   0.000 Y -  -0.05 (2.09) -2.4%\n",
      "wikitext.revision.diff.markup_prop_delta_decrease                       continuous   0.000 Y Y   0.03 (-0.08) -34.6%\n",
      "english.dictionary.revision.diff.non_dict_word_prop_delta_sum           continuous   0.001 - -  -0.11 (1.34) -7.9%\n",
      "wikitext.revision.diff.number_prop_delta_sum                            continuous   0.001 - -  -0.25 (1.13) -21.9%\n",
      "revision.diff.longest_new_repeated_char                                 continuous   0.002 - -   0.40 (1.19) 33.4%\n",
      "wikitext.revision.diff.uppercase_word_prop_delta_increase               continuous   0.011 - -  -0.24 (0.61) -39.9%\n",
      "english.dictionary.revision.diff.dict_word_delta_decrease               continuous   0.027 - -  -1.87 (-11.60) 16.1%\n",
      "english.dictionary.revision.diff.dict_word_prop_delta_decrease          continuous   0.027 - -   0.30 (-3.89) -7.8%\n",
      "wikitext.revision.diff.uppercase_word_prop_delta_sum                    continuous   0.033 - -  -0.23 (0.47) -49.9%\n",
      "wikitext.revision.diff.number_delta_sum                                 continuous   0.060 - -   0.12 (0.98) 11.9%\n",
      "revision.diff.headings_change                                           continuous   0.150 - -  -0.03 (0.05) -59.2%\n",
      "english.dictionary.revision.diff.non_dict_word_delta_decrease           continuous   1.517 - -  -0.49 (-1.60) 30.2%\n",
      "english.dictionary.revision.diff.non_dict_word_delta_increase           continuous   2.412 - -   0.48 (2.82) 16.8%\n",
      "english.dictionary.revision.diff.non_dict_word_delta_sum                continuous   4.258 - -  -0.01 (1.22) -0.8%\n",
      "english.dictionary.revision.diff.non_dict_word_prop_delta_decrease      continuous   4.930 - -  -0.06 (-0.74) 7.4%\n",
      "wikitext.revision.diff.markup_delta_decrease                            continuous  21.719 - -  -0.61 (-2.60) 23.5%\n",
      "revision.diff.tags_change                                               continuous  29.784 - -  -0.04 (0.73) -5.6%\n",
      "wikitext.revision.diff.uppercase_word_prop_delta_decrease               continuous  44.384 - -   0.01 (-0.14) -7.1%\n",
      "revision.diff.templates_change                                          continuous  55.005 - -   0.01 (0.23) 4.4%\n",
      "english.informals.revision.diff.match_prop_delta_increase               continuous  67.120 - -   0.00 (0.06) 7.7%\n",
      "revision.diff.ref_tags_change                                           continuous  67.310 - -  -0.04 (0.12) -30.6%\n",
      "english.informals.revision.diff.match_prop_delta_sum                    continuous  69.626 - -   0.00 (0.04) 8.3%\n",
      "revision.diff.external_links_change                                     continuous  71.616 - -   0.01 (0.09) 16.0%\n",
      "wikitext.revision.diff.uppercase_word_delta_increase                    continuous  76.069 - -  -0.15 (0.76) -19.5%\n",
      "wikitext.revision.diff.uppercase_word_delta_sum                         continuous  79.937 - -  -0.20 (0.44) -45.9%\n",
      "english.informals.revision.diff.match_delta_decrease                    continuous  80.000 - -  -0.03 (-0.05) 51.9%\n",
      "english.informals.revision.diff.match_prop_delta_decrease               continuous  80.000 - -  -0.00 (-0.02) 6.5%\n",
      "english.informals.revision.diff.match_delta_sum                         continuous  80.000 - -  -0.02 (0.05) -29.5%\n",
      "english.badwords.revision.diff.match_delta_sum                          continuous  80.000 - -  -0.03 (0.00) -704.4%\n",
      "english.badwords.revision.diff.match_delta_decrease                     continuous  80.000 - -  -0.04 (-0.01) 246.3%\n",
      "wikitext.revision.diff.uppercase_word_delta_decrease                    continuous  80.000 - -  -0.05 (-0.33) 16.1%\n",
      "english.badwords.revision.diff.match_prop_delta_decrease                continuous  80.000 - -   0.00 (-0.01) -11.0%\n",
      "english.informals.revision.diff.match_delta_increase                    continuous  80.000 - -   0.01 (0.11) 9.8%\n",
      "english.badwords.revision.diff.match_prop_delta_increase                continuous  80.000 - -  -0.00 (0.02) -5.9%\n",
      "english.badwords.revision.diff.match_delta_increase                     continuous  80.000 - -   0.00 (0.02) 4.0%\n",
      "english.badwords.revision.diff.match_prop_delta_sum                     continuous  80.000 - -  -0.00 (0.01) -0.5%\n",
      "revision.comment.has_link                                                   binary   0.000 Y Y  -0.07 (0.24) -30.1%\n",
      "revision.user.is_patroller                                                  binary   0.000 Y Y  -0.06 (0.30) -19.1%\n",
      "revision.comment.suggests_section_edit                                      binary   0.000 Y Y   0.06 (0.39) 15.8%\n",
      "revision.user.is_bot                                                        binary   0.000 Y Y  -0.02 (0.06) -30.3%\n",
      "revision.user.is_admin                                                      binary   0.000 Y Y  -0.02 (0.07) -29.2%\n",
      "revision.user.is_anon                                                       binary   0.000 Y Y   0.04 (0.24) 15.7%\n",
      "revision.user.is_trusted                                                    binary   0.911 - -   0.00 (0.02) 17.1%\n",
      "revision.user.has_advanced_rights                                           binary   2.814 - -   0.00 (0.00) 33.1%\n",
      "revision.user.is_curator                                                    binary  36.982 - -   0.00 (0.02) 4.4%\n",
      "revision.page.is_draftspace                                                 binary  80.000 - Y   0.00 (0.00) inf%\n",
      "revision.page.is_mainspace                                                  binary  80.000 - Y  -0.00 (1.00) -0.0%\n",
      "revision.page.is_articleish                                                 binary  80.000 - -  -0.00 (1.00) -0.0%\n"
     ]
    }
   ],
   "source": [
    "n_features = len(sdf.columns)\n",
    "univariate_results = []\n",
    "for col_name, col_dtype in zip(sdf.columns, sdf.dtypes):\n",
    "    is_feature_binary = False\n",
    "    std = 0\n",
    "    if col_dtype == bool:\n",
    "        is_feature_binary  = True\n",
    "        s_true_count = np.sum(sdf[col_name])\n",
    "        s_false_count = len(sdf) - s_true_count\n",
    "        t_true_count = np.sum(tdf[col_name])\n",
    "        t_false_count = len(tdf) - t_true_count\n",
    "        if s_true_count == 0 or s_false_count == 0 or t_true_count == 0 or t_false_count == 0:\n",
    "            p = n_features\n",
    "            diff = (t_true_count / len(tdf)) - (s_true_count / len(sdf))\n",
    "        else:\n",
    "            cont = np.array(\n",
    "                [[s_true_count, s_false_count],\n",
    "                 [t_true_count, t_false_count]]\n",
    "            )\n",
    "            chi2, p, dof, expctd = scipy.stats.chi2_contingency(cont)\n",
    "            p = p * n_features\n",
    "        diff = (t_true_count / len(tdf)) - (s_true_count / len(sdf))\n",
    "    else:\n",
    "        D, p = scipy.stats.ks_2samp(sdf[col_name], tdf[col_name])\n",
    "        p = p * n_features\n",
    "        diff = np.mean(tdf[col_name]) - np.mean(sdf[col_name])\n",
    "        std = np.std(np.concatenate((sdf[col_name], tdf[col_name])))\n",
    "    pre_mean = np.mean(sdf[col_name])\n",
    "    _, mean_diff_p = scipy.stats.ttest_ind(sdf[col_name], tdf[col_name], equal_var=False)\n",
    "    mean_diff_p *= n_features\n",
    "    tup = (col_name, is_feature_binary, p, diff, std, pre_mean, diff / pre_mean, mean_diff_p)\n",
    "    univariate_results.append(tup)\n",
    "univariate_results.sort(key=lambda tup: (tup[1], abs(tup[2]), -abs(tup[6])), reverse=False)\n",
    "print(f\"{'Feature Name':<71} {'Feat. Type':>10} {'p-value':>7} {'S'} {'T'} {'Diff':>6}\")\n",
    "print(\"=\"*100)\n",
    "for tup in univariate_results:\n",
    "    col_name, is_feature_binary, p, diff, std, pre_mean, pct_change, mean_diff_p = tup\n",
    "    print(f\"{col_name[8:]:<71} {'binary' if is_feature_binary else 'continuous':>10} {p:7.3f} {'Y' if p < 0.001 else '-'} {'Y' if mean_diff_p < 0.001 else '-'} {diff:6.2f} ({pre_mean:.2f}) {pct_change*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate testing\n",
    "\n",
    "Multivariate kernel two-sample tests\n",
    "\n",
    "To get a p-value directly comparing the two samples, using Maximum Mean Discrepancy or some other test.\n",
    "\n",
    "TODO Do this in R lol, no fast Python implementation exists as far as I can tell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute level of difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KLdivergence(x, y):\n",
    "    \"\"\"Compute the Kullback-Leibler divergence between two multivariate samples.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : 2D array (n,d)\n",
    "    Samples from distribution P, which typically represents the true\n",
    "    distribution.\n",
    "    y : 2D array (m,d)\n",
    "    Samples from distribution Q, which typically represents the approximate\n",
    "    distribution.\n",
    "    Returns\n",
    "    -------\n",
    "    out : float\n",
    "    The estimated Kullback-Leibler divergence D(P||Q).\n",
    "    References\n",
    "    ----------\n",
    "    Pérez-Cruz, F. Kullback-Leibler divergence estimation of\n",
    "    continuous distributions IEEE International Symposium on Information\n",
    "    Theory, 2008.\n",
    "    \n",
    "    https://gist.github.com/atabakd/ed0f7581f8510c8587bc2f41a094b518\n",
    "    \"\"\"\n",
    "    from scipy.spatial import cKDTree as KDTree\n",
    "\n",
    "    # Check the dimensions are consistent\n",
    "    x = np.atleast_2d(x)\n",
    "    y = np.atleast_2d(y)\n",
    "\n",
    "    n,d = x.shape\n",
    "    m,dy = y.shape\n",
    "\n",
    "    assert(d == dy)\n",
    "\n",
    "\n",
    "    # Build a KD tree representation of the samples and find the nearest neighbour\n",
    "    # of each point in x.\n",
    "    xtree = KDTree(x)\n",
    "    ytree = KDTree(y)\n",
    "\n",
    "    # Get the first two nearest neighbours for x, since the closest one is the\n",
    "    # sample itself.\n",
    "    r = xtree.query(x, k=2, eps=.01, p=2)[0][:,1]\n",
    "    s = ytree.query(x, k=1, eps=.01, p=2)[0]\n",
    "\n",
    "    # There is a mistake in the paper. In Eq. 14, the right side misses a negative sign\n",
    "    # on the first term of the right hand side.\n",
    "    return -np.log(r/s).sum() * d / n + np.log(m / (n - 1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13529, 80)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:03:47.998024\n",
      "2.02 w std. 0.50\n"
     ]
    }
   ],
   "source": [
    "# KL divergence comparing the training data to the April 2015 data\n",
    "tdf = month_sample_df.drop(columns='damaging')\n",
    "start = datetime.now()\n",
    "kl_sample_n = 10000\n",
    "n_iters = 10\n",
    "kld_list = []\n",
    "for i in range(n_iters):\n",
    "    kld = KLdivergence(sdf.sample(n=kl_sample_n).to_numpy(), (tdf.sample(n=kl_sample_n) + 0.0000000001).to_numpy())\n",
    "    kld_list.append(kld)\n",
    "print(f\"{datetime.now() - start}\")\n",
    "print(f\"{np.mean(kld_list):.2f} w std. {np.std(kld_list):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:04:16.450829\n",
      "6.37 w std. 0.31\n"
     ]
    }
   ],
   "source": [
    "# KL divergence comparing the training data to the sample1 data\n",
    "tdf = mock_features_df.drop(columns='damaging')\n",
    "start = datetime.now()\n",
    "kl_sample_n = 10000\n",
    "n_iters = 10\n",
    "kld_list = []\n",
    "for i in range(n_iters):\n",
    "    kld = KLdivergence(sdf.sample(n=kl_sample_n).to_numpy(), (tdf.sample(n=kl_sample_n) + 0.0000000001).to_numpy())\n",
    "    kld_list.append(kld)\n",
    "print(f\"{datetime.now() - start}\")\n",
    "print(f\"{np.mean(kld_list):.2f} w std. {np.std(kld_list):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Flagon Python3",
   "language": "python",
   "name": "flagon-conda-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
